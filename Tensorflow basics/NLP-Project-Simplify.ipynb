{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/davalpez/MyKaggle/blob/master/Tensorflow%20basics/NLP-Project-Simplify.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project : Simplify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a project that is part of the Tensorflow Deep Learning bootcamp from Udemy.\n",
    "\n",
    "**Overview**\n",
    "\n",
    "The objective of this project is to create a NLP model that transforms convoluted written abstract into an easier readable format.\n",
    "\n",
    "The data user will be taken from this source:\n",
    "https://arxiv.org/abs/1710.06071\n",
    "The model used in this project will be based on:\n",
    "https://arxiv.org/pdf/1612.05251\n",
    "\n",
    "**Index guide for the notebook**\n",
    "\n",
    "1. Notebook setup\n",
    "2. Fetch dataset\n",
    "3. Create preprocess functions\n",
    "4. Draft model experimentation\n",
    "5. Multimodal model creation\n",
    "6. Visualize predictions\n",
    "7. Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "\n",
    "As we have done before, we will first import our own library file where we store functions introduced in other notebooks to continue more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../lib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'modelgraph' from '/home/david/VSpython/Git/lib/modelgraph.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import modelgraph  # Import the module\n",
    "importlib.reload(modelgraph)  # Reload it after changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 27 23:16:59 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.51                 Driver Version: 561.19         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2070 ...    On  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   68C    P0             18W /   80W |     303MiB /   8192MiB |     28%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A        31      G   /Xwayland                                   N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.set_soft_device_placement(True)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export TF_GPU_ALLOCATOR=cuda_malloc_async"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we will use is fetched from the this [pubmed-rct repository](https://github.com/Franck-Dernoncourt/pubmed-rct)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git ../Sources/pubmed-rct  \n",
    "!ls ../Sources/pubmed-rct  \n",
    "\n",
    "Cloning into '../Sources/pubmed-rct'...\n",
    "\n",
    "PubMed_200k_RCT  \n",
    "PubMed_200k_RCT_numbers_replaced_with_at_sign  \n",
    "PubMed_20k_RCT  \n",
    "PubMed_20k_RCT_numbers_replaced_with_at_sign  \n",
    "README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the content of the file, we have :\n",
    "* `PubMed 20k` as a lite version of the `PubMed 200k`.\n",
    "* `PubMed 200k` as the main file containing the abstracts.\n",
    "*  Copies of `PubMed 200k` and `PubMed 20k` where numbers had been replaced with the `@` symbol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start using the `PubMed 20k replaced with at sign` dataset, as it will be a good representation to train and test our models and figure out what model works best with this type of data. As we progress with our experiments, we can later scale to the `PubMed 200k replaced with at sign`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev.txt  test.txt  train.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check what files are in the PubMed_20K dataset \n",
    "!ls ../Sources/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how we already have the dataset separated into train and test files. We also have dev file, which is another way of naming the validation file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadLinesFromFile(file_path):\n",
    "    \"\"\"\n",
    "    Reads lines from a file and returns them as a list.\n",
    "\n",
    "    Args:\n",
    "    file_path (str): The path to the file to be read.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of strings, each representing a line from the file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['###24293578\\n',\n",
       " 'OBJECTIVE\\tTo investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .\\n',\n",
       " 'METHODS\\tA total of @ patients with primary knee OA were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .\\n',\n",
       " 'METHODS\\tOutcome measures included pain reduction and improvement in function scores and systemic inflammation markers .\\n',\n",
       " 'METHODS\\tPain was assessed using the visual analog pain scale ( @-@ mm ) .\\n',\n",
       " 'METHODS\\tSecondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and @-min walk distance ( @MWD ) .\\n',\n",
       " 'METHODS\\tSerum levels of interleukin @ ( IL-@ ) , IL-@ , tumor necrosis factor ( TNF ) - , and high-sensitivity C-reactive protein ( hsCRP ) were measured .\\n',\n",
       " 'RESULTS\\tThere was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , PGA , and @MWD at @ weeks .\\n',\n",
       " 'RESULTS\\tThe mean difference between treatment arms ( @ % CI ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .\\n',\n",
       " 'RESULTS\\tFurther , there was a clinically relevant reduction in the serum levels of IL-@ , IL-@ , TNF - , and hsCRP at @ weeks in the intervention group when compared to the placebo group .\\n',\n",
       " 'RESULTS\\tThese differences remained significant at @ weeks .\\n',\n",
       " 'RESULTS\\tThe Outcome Measures in Rheumatology Clinical Trials-Osteoarthritis Research Society International responder rate was @ % in the intervention group and @ % in the placebo group ( p < @ ) .\\n',\n",
       " 'CONCLUSIONS\\tLow-dose oral prednisolone had both a short-term and a longer sustained effect resulting in less knee pain , better physical function , and attenuation of systemic inflammation in older patients with knee OA ( ClinicalTrials.gov identifier NCT@ ) .\\n',\n",
       " '\\n',\n",
       " '###24854809\\n',\n",
       " 'BACKGROUND\\tEmotional eating is associated with overeating and the development of obesity .\\n',\n",
       " 'BACKGROUND\\tYet , empirical evidence for individual ( trait ) differences in emotional eating and cognitive mechanisms that contribute to eating during sad mood remain equivocal .\\n',\n",
       " 'OBJECTIVE\\tThe aim of this study was to test if attention bias for food moderates the effect of self-reported emotional eating during sad mood ( vs neutral mood ) on actual food intake .\\n',\n",
       " 'OBJECTIVE\\tIt was expected that emotional eating is predictive of elevated attention for food and higher food intake after an experimentally induced sad mood and that attentional maintenance on food predicts food intake during a sad versus a neutral mood .\\n',\n",
       " 'METHODS\\tParticipants ( N = @ ) were randomly assigned to one of the two experimental mood induction conditions ( sad/neutral ) .\\n']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_path = '../Sources/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/train.txt'\n",
    "train_raw_data_20k = ReadLinesFromFile(train_path)\n",
    "train_raw_data_20k[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also take a look to the test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['###24845963\\n',\n",
       " 'BACKGROUND\\tThis study analyzed liver function abnormalities in heart failure patients admitted with severe acute decompensated heart failure ( ADHF ) .\\n',\n",
       " 'RESULTS\\tA post hoc analysis was conducted with the use of data from the Evaluation Study of Congestive Heart Failure and Pulmonary Artery Catheterization Effectiveness ( ESCAPE ) .\\n',\n",
       " 'RESULTS\\tLiver function tests ( LFTs ) were measured at @ time points from baseline , at discharge , and up to @ months follow-up .\\n',\n",
       " 'RESULTS\\tSurvival analyses were used to assess the association between admission Model of End-Stage Liver Disease Excluding International Normalized Ratio ( MELD-XI ) scores and patient outcome.There was a high prevalence of abnormal baseline ( admission ) LFTs ( albumin @ % , aspartate transaminase @ % , alanine transaminase @ % , and total bilirubin @ % ) .\\n',\n",
       " \"RESULTS\\tThe percentage of patients with abnormal LFTs decreased significantly from baseline to @-months ' follow-up .\\n\",\n",
       " 'RESULTS\\tWhen mean hemodynamic profiles were compared in patients with abnormal versus normal LFTs , elevated total bilirubin was associated with a significantly lower cardiac index ( @ vs @ ; P < @ ) and higher central venous pressure ( @ vs @ ; P = @ ) .\\n',\n",
       " 'RESULTS\\tMultivariable analyses revealed that patients with elevated MELD-XI scores ( @ ) had a @-fold ( hazard ratio@ @ , @ % confidence interval @-@ @ ) increased risk of death , rehospitalization , or transplantation after adjusting for baseline LFTs , age , sex , race , body mass index , diabetes , and systolic blood pressure .\\n',\n",
       " 'CONCLUSIONS\\tAbnormal LFTs are common in the ADHF population and are a dynamic marker of an impaired hemodynamic state .\\n',\n",
       " 'CONCLUSIONS\\tElevated MELD-XI scores are associated with poor outcomes among patients admitted with ADHF .\\n',\n",
       " '\\n',\n",
       " '###24469619\\n',\n",
       " 'BACKGROUND\\tMinimally invasive endovascular aneurysm repair ( EVAR ) could be a surgical technique that improves outcome of patients with ruptured abdominal aortic aneurysm ( rAAA ) .\\n',\n",
       " 'BACKGROUND\\tThe aim of this study was to analyse the cost-effectiveness and cost-utility of EVAR compared with standard open repair ( OR ) in the treatment of rAAA , with costs per @-day and @-month survivor as outcome parameters .\\n',\n",
       " 'METHODS\\tResource use was determined from the Amsterdam Acute Aneurysm ( AJAX ) trial , a multicentre randomized trial comparing EVAR with OR in patients with rAAA .\\n',\n",
       " 'METHODS\\tThe analysis was performed from a provider perspective .\\n',\n",
       " 'METHODS\\tAll costs were calculated as if all patients had been treated in the same hospital ( Onze Lieve Vrouwe Gasthuis , teaching hospital ) .\\n',\n",
       " 'RESULTS\\tA total of @ patients were randomized .\\n',\n",
       " 'RESULTS\\tThe @-day mortality rate was @ per cent after EVAR and @ per cent for OR : absolute risk reduction ( ARR ) @ ( @ per cent confidence interval ( c.i. ) -@ to @ ) per cent .\\n',\n",
       " 'RESULTS\\tAt @months , the total mortality rate for EVAR was @ per cent , compared with @ per cent among those assigned to OR : ARR @ ( -@ to @ ) per cent .\\n']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_path = '../Sources/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/test.txt'\n",
    "test_raw_data_20k = ReadLinesFromFile(test_path)\n",
    "test_raw_data_20k[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['###24290286\\n',\n",
       " 'BACKGROUND\\tIgE sensitization to Aspergillus fumigatus and a positive sputum fungal culture result are common in patients with refractory asthma .\\n',\n",
       " 'BACKGROUND\\tIt is not clear whether these patients would benefit from antifungal treatment .\\n',\n",
       " 'OBJECTIVE\\tWe sought to determine whether a @-month course of voriconazole improved asthma-related outcomes in patients with asthma who are IgE sensitized to A fumigatus .\\n',\n",
       " 'METHODS\\tAsthmatic patients who were IgE sensitized to A fumigatus with a history of at least @ severe exacerbations in the previous @ months were treated for @ months with @ mg of voriconazole twice daily , followed by observation for @ months , in a double-blind , placebo-controlled , randomized design .\\n',\n",
       " 'METHODS\\tPrimary outcomes were improvement in quality of life at the end of the treatment period and a reduction in the number of severe exacerbations over the @ months of the study .\\n',\n",
       " 'RESULTS\\tSixty-five patients were randomized .\\n',\n",
       " 'RESULTS\\tFifty-nine patients started treatment ( @ receiving voriconazole and @ receiving placebo ) and were included in an intention-to-treat analysis .\\n',\n",
       " 'RESULTS\\tFifty-six patients took the full @ months of medication .\\n',\n",
       " 'RESULTS\\tBetween the voriconazole and placebo groups , there were no significant differences in the number of severe exacerbations ( @ vs @ per patient per year , respectively ; mean difference , @ ; @ % CI , @-@ @ ) , quality of life ( change in Asthma Quality of Life Questionnaire score , @ vs @ ; mean difference between groups , @ ; @ % CI , -@ to -@ ) , or any of our secondary outcome measures .\\n']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_path = '../Sources/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/dev.txt'\n",
    "val_raw_data_20k = ReadLinesFromFile(val_path)\n",
    "val_raw_data_20k[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way the data is structured in this file is :\n",
    "\n",
    "* A hashtag line indicating a new abstract `###`\n",
    "* A `ID number` for an specific abstract.\n",
    "* A `type` for the content of every line.\n",
    "* A `text`that indentifies with the content of every line.\n",
    "* An `ES`after every line.\n",
    "\n",
    "How it would be interesting to format this data into a dictionary per abstract.\n",
    "\n",
    "1. Line number we are refering from an specific abstract.\n",
    "2. Specify the type of text.\n",
    "3. Displaying the text content\n",
    "4. Total lines.\n",
    "5. Abstract ID, so we can see and compare the results better after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'type': 'OBJECTIVE',\n",
       "   'text': 'To investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .',\n",
       "   'line_number': 1,\n",
       "   'total_lines': 12,\n",
       "   'abstract_id': '24293578'},\n",
       "  {'type': 'METHODS',\n",
       "   'text': 'A total of @ patients with primary knee OA were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .',\n",
       "   'line_number': 2,\n",
       "   'total_lines': 12,\n",
       "   'abstract_id': '24293578'},\n",
       "  {'type': 'METHODS',\n",
       "   'text': 'Outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .',\n",
       "   'line_number': 3,\n",
       "   'total_lines': 12,\n",
       "   'abstract_id': '24293578'},\n",
       "  {'type': 'METHODS',\n",
       "   'text': 'Pain was assessed using the visual analog pain scale ( @-@ mm ) .',\n",
       "   'line_number': 4,\n",
       "   'total_lines': 12,\n",
       "   'abstract_id': '24293578'},\n",
       "  {'type': 'METHODS',\n",
       "   'text': 'Secondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and @-min walk distance ( @MWD ) .',\n",
       "   'line_number': 5,\n",
       "   'total_lines': 12,\n",
       "   'abstract_id': '24293578'},\n",
       "  {'type': 'METHODS',\n",
       "   'text': 'Serum levels of interleukin @ ( IL-@ ) , IL-@ , tumor necrosis factor ( TNF ) - , and high-sensitivity C-reactive protein ( hsCRP ) were measured .',\n",
       "   'line_number': 6,\n",
       "   'total_lines': 12,\n",
       "   'abstract_id': '24293578'},\n",
       "  {'type': 'RESULTS',\n",
       "   'text': 'There was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , PGA , and @MWD at @ weeks .',\n",
       "   'line_number': 7,\n",
       "   'total_lines': 12,\n",
       "   'abstract_id': '24293578'},\n",
       "  {'type': 'RESULTS',\n",
       "   'text': 'The mean difference between treatment arms ( @ % CI ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .',\n",
       "   'line_number': 8,\n",
       "   'total_lines': 12,\n",
       "   'abstract_id': '24293578'},\n",
       "  {'type': 'RESULTS',\n",
       "   'text': 'Further , there was a clinically relevant reduction in the serum levels of IL-@ , IL-@ , TNF - , and hsCRP at @ weeks in the intervention group when compared to the placebo group .',\n",
       "   'line_number': 9,\n",
       "   'total_lines': 12,\n",
       "   'abstract_id': '24293578'},\n",
       "  {'type': 'RESULTS',\n",
       "   'text': 'These differences remained significant at @ weeks .',\n",
       "   'line_number': 10,\n",
       "   'total_lines': 12,\n",
       "   'abstract_id': '24293578'},\n",
       "  {'type': 'RESULTS',\n",
       "   'text': 'The Outcome Measures in Rheumatology Clinical Trials-Osteoarthritis Research Society International responder rate was @ % in the intervention group and @ % in the placebo group ( p < @ ) .',\n",
       "   'line_number': 11,\n",
       "   'total_lines': 12,\n",
       "   'abstract_id': '24293578'},\n",
       "  {'type': 'CONCLUSIONS',\n",
       "   'text': 'Low-dose oral prednisolone had both a short-term and a longer sustained effect resulting in less knee pain , better physical function , and attenuation of systemic inflammation in older patients with knee OA ( ClinicalTrials.gov identifier NCT@ ) .',\n",
       "   'line_number': 12,\n",
       "   'total_lines': 12,\n",
       "   'abstract_id': '24293578'}],\n",
       " [{'type': 'BACKGROUND',\n",
       "   'text': 'Emotional eating is associated with overeating and the development of obesity .',\n",
       "   'line_number': 1,\n",
       "   'total_lines': 11,\n",
       "   'abstract_id': '24854809'},\n",
       "  {'type': 'BACKGROUND',\n",
       "   'text': 'Yet , empirical evidence for individual ( trait ) differences in emotional eating and cognitive mechanisms that contribute to eating during sad mood remain equivocal .',\n",
       "   'line_number': 2,\n",
       "   'total_lines': 11,\n",
       "   'abstract_id': '24854809'},\n",
       "  {'type': 'OBJECTIVE',\n",
       "   'text': 'The aim of this study was to test if attention bias for food moderates the effect of self-reported emotional eating during sad mood ( vs neutral mood ) on actual food intake .',\n",
       "   'line_number': 3,\n",
       "   'total_lines': 11,\n",
       "   'abstract_id': '24854809'},\n",
       "  {'type': 'OBJECTIVE',\n",
       "   'text': 'It was expected that emotional eating is predictive of elevated attention for food and higher food intake after an experimentally induced sad mood and that attentional maintenance on food predicts food intake during a sad versus a neutral mood .',\n",
       "   'line_number': 4,\n",
       "   'total_lines': 11,\n",
       "   'abstract_id': '24854809'},\n",
       "  {'type': 'METHODS',\n",
       "   'text': 'Participants ( N = @ ) were randomly assigned to one of the two experimental mood induction conditions ( sad/neutral ) .',\n",
       "   'line_number': 5,\n",
       "   'total_lines': 11,\n",
       "   'abstract_id': '24854809'},\n",
       "  {'type': 'METHODS',\n",
       "   'text': 'Attentional biases for high caloric foods were measured by eye tracking during a visual probe task with pictorial food and neutral stimuli .',\n",
       "   'line_number': 6,\n",
       "   'total_lines': 11,\n",
       "   'abstract_id': '24854809'},\n",
       "  {'type': 'METHODS',\n",
       "   'text': 'Self-reported emotional eating was assessed with the Dutch Eating Behavior Questionnaire ( DEBQ ) and ad libitum food intake was tested by a disguised food offer .',\n",
       "   'line_number': 7,\n",
       "   'total_lines': 11,\n",
       "   'abstract_id': '24854809'},\n",
       "  {'type': 'RESULTS',\n",
       "   'text': 'Hierarchical multivariate regression modeling showed that self-reported emotional eating did not account for changes in attention allocation for food or food intake in either condition .',\n",
       "   'line_number': 8,\n",
       "   'total_lines': 11,\n",
       "   'abstract_id': '24854809'},\n",
       "  {'type': 'RESULTS',\n",
       "   'text': 'Yet , attention maintenance on food cues was significantly related to increased intake specifically in the neutral condition , but not in the sad mood condition .',\n",
       "   'line_number': 9,\n",
       "   'total_lines': 11,\n",
       "   'abstract_id': '24854809'},\n",
       "  {'type': 'CONCLUSIONS',\n",
       "   'text': 'The current findings show that self-reported emotional eating ( based on the DEBQ ) might not validly predict who overeats when sad , at least not in a laboratory setting with healthy women .',\n",
       "   'line_number': 10,\n",
       "   'total_lines': 11,\n",
       "   'abstract_id': '24854809'},\n",
       "  {'type': 'CONCLUSIONS',\n",
       "   'text': 'Results further suggest that attention maintenance on food relates to eating motivation when in a neutral affective state , and might therefore be a cognitive mechanism contributing to increased food intake in general , but maybe not during sad mood .',\n",
       "   'line_number': 11,\n",
       "   'total_lines': 11,\n",
       "   'abstract_id': '24854809'}]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def GenerateAbstractSamples(raw_data):\n",
    "    \"\"\"\n",
    "    Generates a prcoessed sampled dataset from a raw dataset. The function takes in a variable\n",
    "    raw dataset and returns a list of dictionaries, where each dictionary represents an abstract\n",
    "    sample.\n",
    "\n",
    "    Args:\n",
    "    dataset (list): A list of strings, each representing a line from the dataset file.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of abstracts, where each abstract is represented as a list of dictionaries. \n",
    "          Each dictionary contains the following keys:\n",
    "          - 'type': The type of the line (e.g., BACKGROUND, METHODS, RESULTS, CONCLUSIONS).\n",
    "          - 'text': The text content of the line.\n",
    "          - 'line_number': The line number within the abstract.\n",
    "          - 'total_lines': The total number of lines in the abstract.\n",
    "          - 'abstract_id': The ID of the abstract.\n",
    "    \"\"\"\n",
    "    abstracts = []\n",
    "    current_abstract = None\n",
    "    line_count = 0\n",
    "    abstract_id = None\n",
    "\n",
    "    for line in raw_data:\n",
    "        if line.startswith('###'):\n",
    "            if current_abstract:\n",
    "                for entry in current_abstract:\n",
    "                    entry['total_lines'] = line_count\n",
    "                    entry['abstract_id'] = abstract_id\n",
    "                abstracts.append(current_abstract)\n",
    "            current_abstract = []\n",
    "            line_count = 0\n",
    "            abstract_id = line.strip()[3:]  # Extract the abstract ID\n",
    "        elif line.strip():\n",
    "            line_count += 1\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                type_, text = parts\n",
    "                current_abstract.append({\n",
    "                    'type': type_,\n",
    "                    'text': text.strip(),\n",
    "                    'line_number': line_count,\n",
    "                    'total_lines': 0,  # This will be updated later\n",
    "                    'abstract_id': abstract_id  # This will be updated later\n",
    "                })\n",
    "\n",
    "    if current_abstract:\n",
    "        for entry in current_abstract:\n",
    "            entry['total_lines'] = line_count\n",
    "            entry['abstract_id'] = abstract_id\n",
    "        abstracts.append(current_abstract)\n",
    "\n",
    "    return abstracts\n",
    "\n",
    "# Generate abstracts from the test dataset\n",
    "train_samples_20k = GenerateAbstractSamples(train_raw_data_20k)\n",
    "train_samples_20k[:2]  # Print the first 2 abstracts for verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is looking good now as we have the data now properly organizded and in a format that is easier to extract. We can also transform it into a pandas dataframe for better visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples_20k = GenerateAbstractSamples(test_raw_data_20k)\n",
    "val_samples_20k = GenerateAbstractSamples(val_raw_data_20k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 2500, 2500)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_samples_20k), len(val_samples_20k), len(test_samples_20k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def CreateSampleDf(Samples):\n",
    "    # Create a dataframe for each sample in Samples\n",
    "    sample_dfs = [pd.DataFrame(sample) for sample in Samples]\n",
    "\n",
    "    # Combine all the dataframes into a single dataframe\n",
    "    combined_df = pd.concat(sample_dfs, ignore_index=True)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_20k = CreateSampleDf(train_samples_20k)\n",
    "val_df_20k = CreateSampleDf(val_samples_20k)\n",
    "test_df_20k = CreateSampleDf(test_samples_20k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "line_number",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "total_lines",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "abstract_id",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "9582b656-187c-4b66-a268-8f13a709cffe",
       "rows": [
        [
         "0",
         "OBJECTIVE",
         "To investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .",
         "1",
         "12",
         "24293578"
        ],
        [
         "1",
         "METHODS",
         "A total of @ patients with primary knee OA were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .",
         "2",
         "12",
         "24293578"
        ],
        [
         "2",
         "METHODS",
         "Outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .",
         "3",
         "12",
         "24293578"
        ],
        [
         "3",
         "METHODS",
         "Pain was assessed using the visual analog pain scale ( @-@ mm ) .",
         "4",
         "12",
         "24293578"
        ],
        [
         "4",
         "METHODS",
         "Secondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and @-min walk distance ( @MWD ) .",
         "5",
         "12",
         "24293578"
        ],
        [
         "5",
         "METHODS",
         "Serum levels of interleukin @ ( IL-@ ) , IL-@ , tumor necrosis factor ( TNF ) - , and high-sensitivity C-reactive protein ( hsCRP ) were measured .",
         "6",
         "12",
         "24293578"
        ],
        [
         "6",
         "RESULTS",
         "There was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , PGA , and @MWD at @ weeks .",
         "7",
         "12",
         "24293578"
        ],
        [
         "7",
         "RESULTS",
         "The mean difference between treatment arms ( @ % CI ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .",
         "8",
         "12",
         "24293578"
        ],
        [
         "8",
         "RESULTS",
         "Further , there was a clinically relevant reduction in the serum levels of IL-@ , IL-@ , TNF - , and hsCRP at @ weeks in the intervention group when compared to the placebo group .",
         "9",
         "12",
         "24293578"
        ],
        [
         "9",
         "RESULTS",
         "These differences remained significant at @ weeks .",
         "10",
         "12",
         "24293578"
        ],
        [
         "10",
         "RESULTS",
         "The Outcome Measures in Rheumatology Clinical Trials-Osteoarthritis Research Society International responder rate was @ % in the intervention group and @ % in the placebo group ( p < @ ) .",
         "11",
         "12",
         "24293578"
        ],
        [
         "11",
         "CONCLUSIONS",
         "Low-dose oral prednisolone had both a short-term and a longer sustained effect resulting in less knee pain , better physical function , and attenuation of systemic inflammation in older patients with knee OA ( ClinicalTrials.gov identifier NCT@ ) .",
         "12",
         "12",
         "24293578"
        ],
        [
         "12",
         "BACKGROUND",
         "Emotional eating is associated with overeating and the development of obesity .",
         "1",
         "11",
         "24854809"
        ],
        [
         "13",
         "BACKGROUND",
         "Yet , empirical evidence for individual ( trait ) differences in emotional eating and cognitive mechanisms that contribute to eating during sad mood remain equivocal .",
         "2",
         "11",
         "24854809"
        ],
        [
         "14",
         "OBJECTIVE",
         "The aim of this study was to test if attention bias for food moderates the effect of self-reported emotional eating during sad mood ( vs neutral mood ) on actual food intake .",
         "3",
         "11",
         "24854809"
        ],
        [
         "15",
         "OBJECTIVE",
         "It was expected that emotional eating is predictive of elevated attention for food and higher food intake after an experimentally induced sad mood and that attentional maintenance on food predicts food intake during a sad versus a neutral mood .",
         "4",
         "11",
         "24854809"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 16
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>total_lines</th>\n",
       "      <th>abstract_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>To investigate the efficacy of @ weeks of dail...</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>24293578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>A total of @ patients with primary knee OA wer...</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>24293578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>Outcome measures included pain reduction and i...</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>24293578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>Pain was assessed using the visual analog pain...</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>24293578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>Secondary outcome measures included the Wester...</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>24293578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>Serum levels of interleukin @ ( IL-@ ) , IL-@ ...</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>24293578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>There was a clinically relevant reduction in t...</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>24293578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>The mean difference between treatment arms ( @...</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>24293578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>Further , there was a clinically relevant redu...</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>24293578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>These differences remained significant at @ we...</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>24293578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>The Outcome Measures in Rheumatology Clinical ...</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>24293578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CONCLUSIONS</td>\n",
       "      <td>Low-dose oral prednisolone had both a short-te...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>24293578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>Emotional eating is associated with overeating...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>24854809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>Yet , empirical evidence for individual ( trai...</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>24854809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>The aim of this study was to test if attention...</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>24854809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>It was expected that emotional eating is predi...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>24854809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           type                                               text  \\\n",
       "0     OBJECTIVE  To investigate the efficacy of @ weeks of dail...   \n",
       "1       METHODS  A total of @ patients with primary knee OA wer...   \n",
       "2       METHODS  Outcome measures included pain reduction and i...   \n",
       "3       METHODS  Pain was assessed using the visual analog pain...   \n",
       "4       METHODS  Secondary outcome measures included the Wester...   \n",
       "5       METHODS  Serum levels of interleukin @ ( IL-@ ) , IL-@ ...   \n",
       "6       RESULTS  There was a clinically relevant reduction in t...   \n",
       "7       RESULTS  The mean difference between treatment arms ( @...   \n",
       "8       RESULTS  Further , there was a clinically relevant redu...   \n",
       "9       RESULTS  These differences remained significant at @ we...   \n",
       "10      RESULTS  The Outcome Measures in Rheumatology Clinical ...   \n",
       "11  CONCLUSIONS  Low-dose oral prednisolone had both a short-te...   \n",
       "12   BACKGROUND  Emotional eating is associated with overeating...   \n",
       "13   BACKGROUND  Yet , empirical evidence for individual ( trai...   \n",
       "14    OBJECTIVE  The aim of this study was to test if attention...   \n",
       "15    OBJECTIVE  It was expected that emotional eating is predi...   \n",
       "\n",
       "    line_number  total_lines abstract_id  \n",
       "0             1           12    24293578  \n",
       "1             2           12    24293578  \n",
       "2             3           12    24293578  \n",
       "3             4           12    24293578  \n",
       "4             5           12    24293578  \n",
       "5             6           12    24293578  \n",
       "6             7           12    24293578  \n",
       "7             8           12    24293578  \n",
       "8             9           12    24293578  \n",
       "9            10           12    24293578  \n",
       "10           11           12    24293578  \n",
       "11           12           12    24293578  \n",
       "12            1           11    24854809  \n",
       "13            2           11    24854809  \n",
       "14            3           11    24854809  \n",
       "15            4           11    24854809  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_20k.head(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see now in this dataframe we have all the abstracts together and we can see the separation by seeing the change in the `abstract_id` or `total_lines` changing. We could create a dataframe where we have each sample saved as well, but we have decided to go this way for now to visualize some metrics better as we are showing in the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type\n",
       "METHODS        59353\n",
       "RESULTS        57953\n",
       "CONCLUSIONS    27168\n",
       "BACKGROUND     21727\n",
       "OBJECTIVE      13839\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Distribution of labels in training data\n",
    "train_df_20k.type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Frequency'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAGeCAYAAACJuDVEAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAANd9JREFUeJzt3X9UlHXe//EXIIP4Y8ZQAVlRKU0jf62oONuPe11ZR6VOpu3RsiSjujV0VTJ/7LpY3Z1s7VTa7Q+2bVfcs7kqe6dbsmIuKu4maWLkj2+SmYUuDFoJo6SAzPX9o5vrdsLykrAZ6Pk45zo51+fN53rPda5zeHXNNR+CDMMwBAAAgG8V7O8GAAAAmgNCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFhAaAIAALCA0AQAAGABoQkAAMCCVv5uoKXwer0qLS1V+/btFRQU5O92AACABYZh6OzZs4qJiVFw8BXuJRl+1L17d0NSg+2xxx4zDMMwzp8/bzz22GNGRESE0bZtW2PcuHGG2+32mePTTz81xowZY4SHhxudO3c25syZY9TW1vrU7Nixw/jxj39s2Gw244YbbjBWr17doJfly5cb3bt3N8LCwoyhQ4cae/bsuar3cuLEicu+FzY2NjY2NrbA306cOHHF3/V+vdP07rvvqq6uznx96NAh/fznP9cvfvELSdLs2bOVk5Oj7OxsORwOTZ8+XePGjdPbb78tSaqrq1NycrKio6O1e/dulZWVafLkyQoNDdWzzz4rSTp+/LiSk5M1depUvfbaa8rLy9PDDz+sLl26yOVySZLWr1+v9PR0ZWZmKjExUUuXLpXL5VJxcbEiIyMtvZf27dtLkk6cOCG73d5k5wgAAFw7Ho9HsbGx5u/xb3VVt1OusZkzZxo33HCD4fV6jYqKCiM0NNTIzs42xz/44ANDklFQUGAYhmH8/e9/N4KDg33uPq1atcqw2+1GdXW1YRiGMXfuXOPmm2/2Oc6ECRMMl8tlvh46dKiRlpZmvq6rqzNiYmKMxYsXW+69srLSkGRUVlZe3ZsGAAB+czW/vwPmQfCamhr9+c9/1kMPPaSgoCAVFhaqtrZWSUlJZk2fPn3UrVs3FRQUSJIKCgrUr18/RUVFmTUul0sej0eHDx82ay6do76mfo6amhoVFhb61AQHByspKcmsuZzq6mp5PB6fDQAAtFwBE5o2bdqkiooKPfjgg5Ikt9stm82mDh06+NRFRUXJ7XabNZcGpvrx+rFvq/F4PDp//rw+++wz1dXVXbamfo7LWbx4sRwOh7nFxsZe9XsGAADNR8CEpj/84Q8aPXq0YmJi/N2KJQsWLFBlZaW5nThxwt8tAQCAaygglhz49NNP9Y9//EOvv/66uS86Olo1NTWqqKjwudtUXl6u6Ohos2bv3r0+c5WXl5tj9f+t33dpjd1uV3h4uEJCQhQSEnLZmvo5LicsLExhYWFX/2YBAECzFBB3mlavXq3IyEglJyeb+xISEhQaGqq8vDxzX3FxsUpKSuR0OiVJTqdTBw8e1KlTp8yabdu2yW63Kz4+3qy5dI76mvo5bDabEhISfGq8Xq/y8vLMGgAAAL/fafJ6vVq9erVSUlLUqtX/teNwOJSamqr09HRFRETIbrdrxowZcjqdGjZsmCRp5MiRio+P1wMPPKAlS5bI7XZr4cKFSktLM+8CTZ06VcuXL9fcuXP10EMPafv27dqwYYNycnLMY6WnpyslJUWDBw/W0KFDtXTpUlVVVWnKlCnf78kAAACB63v4Nt+32rp1qyHJKC4ubjBWv7jlddddZ7Rp08a4++67jbKyMp+aTz75xBg9erQRHh5udOrUyXj88ccvu7jlwIEDDZvNZlx//fWXXdzyv//7v41u3boZNpvNGDp0qPHOO+9c1ftgyQEAAJqfq/n9HWQYhuHn3NYieDweORwOVVZWsrglAADNxNX8/g6IZ5oAAAACHaEJAADAAkITAACABYQmAAAACwhNAAAAFhCaAAAALPD74pZAIOkxP+fKRQHmk+eSr1wEAPjOuNMEAABgAaEJAADAAkITAACABYQmAAAACwhNAAAAFhCaAAAALCA0AQAAWEBoAgAAsIDQBAAAYAGhCQAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFhAaAIAALCA0AQAAGABoQkAAMACQhMAAIAFhCYAAAALCE0AAAAWEJoAAAAsIDQBAABYQGgCAACwgNAEAABgAaEJAADAAkITAACABYQmAAAACwhNAAAAFhCaAAAALCA0AQAAWEBoAgAAsIDQBAAAYAGhCQAAwAK/h6Z///vfuv/++9WxY0eFh4erX79+2rdvnzluGIYyMjLUpUsXhYeHKykpSUePHvWZ44svvtCkSZNkt9vVoUMHpaam6ty5cz41Bw4c0G233abWrVsrNjZWS5YsadBLdna2+vTpo9atW6tfv376+9//fm3eNAAAaHb8GprOnDmjW265RaGhodqyZYv+3//7f3rhhRd03XXXmTVLlizRyy+/rMzMTO3Zs0dt27aVy+XShQsXzJpJkybp8OHD2rZtmzZv3qxdu3bp0UcfNcc9Ho9Gjhyp7t27q7CwUM8//7yefPJJvfLKK2bN7t27de+99yo1NVXvvfeexo4dq7Fjx+rQoUPfz8kAAAABLcgwDMNfB58/f77efvtt/fOf/7zsuGEYiomJ0eOPP645c+ZIkiorKxUVFaWsrCxNnDhRH3zwgeLj4/Xuu+9q8ODBkqTc3FyNGTNGJ0+eVExMjFatWqVf//rXcrvdstls5rE3bdqkI0eOSJImTJigqqoqbd682Tz+sGHDNHDgQGVmZl7xvXg8HjkcDlVWVsput3+n8wL/6TE/x98tXLVPnkv2dwsA0Gxdze9vv95peuONNzR48GD94he/UGRkpH784x/r97//vTl+/Phxud1uJSUlmfscDocSExNVUFAgSSooKFCHDh3MwCRJSUlJCg4O1p49e8ya22+/3QxMkuRyuVRcXKwzZ86YNZcep76m/jhfV11dLY/H47MBAICWy6+h6eOPP9aqVavUq1cvbd26VdOmTdMvf/lLrVmzRpLkdrslSVFRUT4/FxUVZY653W5FRkb6jLdq1UoRERE+NZeb49JjfFNN/fjXLV68WA6Hw9xiY2Ov+v0DAIDmw6+hyev1atCgQXr22Wf14x//WI8++qgeeeQRSx+H+duCBQtUWVlpbidOnPB3SwAA4Brya2jq0qWL4uPjffbddNNNKikpkSRFR0dLksrLy31qysvLzbHo6GidOnXKZ/zixYv64osvfGouN8elx/immvrxrwsLC5PdbvfZAABAy+XX0HTLLbeouLjYZ9+HH36o7t27S5Li4uIUHR2tvLw8c9zj8WjPnj1yOp2SJKfTqYqKChUWFpo127dvl9frVWJiolmza9cu1dbWmjXbtm1T7969zW/qOZ1On+PU19QfBwAA/LD5NTTNnj1b77zzjp599ll99NFHWrt2rV555RWlpaVJkoKCgjRr1iw988wzeuONN3Tw4EFNnjxZMTExGjt2rKSv7kyNGjVKjzzyiPbu3au3335b06dP18SJExUTEyNJuu+++2Sz2ZSamqrDhw9r/fr1WrZsmdLT081eZs6cqdzcXL3wwgs6cuSInnzySe3bt0/Tp0//3s8LAAAIPK38efAhQ4Zo48aNWrBggZ5++mnFxcVp6dKlmjRpklkzd+5cVVVV6dFHH1VFRYVuvfVW5ebmqnXr1mbNa6+9punTp2vEiBEKDg7W+PHj9fLLL5vjDodDb731ltLS0pSQkKBOnTopIyPDZy2nn/zkJ1q7dq0WLlyoX/3qV+rVq5c2bdqkvn37fj8nAwAABDS/rtPUkrBOU8vAOk0A8MPSbNZpAgAAaC4ITQAAABYQmgAAACwgNAEAAFhAaAIAALCA0AQAAGABoQkAAMACQhMAAIAFhCYAAAALCE0AAAAWEJoAAAAsIDQBAABYQGgCAACwgNAEAABgAaEJAADAAkITAACABYQmAAAACwhNAAAAFhCaAAAALCA0AQAAWEBoAgAAsIDQBAAAYAGhCQAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFhAaAIAALCA0AQAAGABoQkAAMACQhMAAIAFhCYAAAALCE0AAAAWEJoAAAAsIDQBAABYQGgCAACwgNAEAABgAaEJAADAAkITAACABYQmAAAAC/wamp588kkFBQX5bH369DHHL1y4oLS0NHXs2FHt2rXT+PHjVV5e7jNHSUmJkpOT1aZNG0VGRuqJJ57QxYsXfWp27typQYMGKSwsTD179lRWVlaDXlasWKEePXqodevWSkxM1N69e6/JewYAAM2T3+803XzzzSorKzO3f/3rX+bY7Nmz9eabbyo7O1v5+fkqLS3VuHHjzPG6ujolJyerpqZGu3fv1po1a5SVlaWMjAyz5vjx40pOTtbw4cNVVFSkWbNm6eGHH9bWrVvNmvXr1ys9PV2LFi3S/v37NWDAALlcLp06der7OQkAACDgBRmGYfjr4E8++aQ2bdqkoqKiBmOVlZXq3Lmz1q5dq3vuuUeSdOTIEd10000qKCjQsGHDtGXLFt1xxx0qLS1VVFSUJCkzM1Pz5s3T6dOnZbPZNG/ePOXk5OjQoUPm3BMnTlRFRYVyc3MlSYmJiRoyZIiWL18uSfJ6vYqNjdWMGTM0f/58S+/F4/HI4XCosrJSdrv9u5wW+FGP+Tn+buGqffJcsr9bAIBm62p+f/v9TtPRo0cVExOj66+/XpMmTVJJSYkkqbCwULW1tUpKSjJr+/Tpo27duqmgoECSVFBQoH79+pmBSZJcLpc8Ho8OHz5s1lw6R31N/Rw1NTUqLCz0qQkODlZSUpJZAwAA0MqfB09MTFRWVpZ69+6tsrIyPfXUU7rtttt06NAhud1u2Ww2dejQwednoqKi5Ha7JUlut9snMNWP1499W43H49H58+d15swZ1dXVXbbmyJEj39h7dXW1qqurzdcej+fq3jwAAGhW/BqaRo8ebf67f//+SkxMVPfu3bVhwwaFh4f7sbMrW7x4sZ566il/twEAAL4nfv947lIdOnTQjTfeqI8++kjR0dGqqalRRUWFT015ebmio6MlSdHR0Q2+TVf/+ko1drtd4eHh6tSpk0JCQi5bUz/H5SxYsECVlZXmduLEiUa9ZwAA0DwEVGg6d+6cjh07pi5duighIUGhoaHKy8szx4uLi1VSUiKn0ylJcjqdOnjwoM+33LZt2ya73a74+Hiz5tI56mvq57DZbEpISPCp8Xq9ysvLM2suJywsTHa73WcDAAAtl19D05w5c5Sfn69PPvlEu3fv1t13362QkBDde++9cjgcSk1NVXp6unbs2KHCwkJNmTJFTqdTw4YNkySNHDlS8fHxeuCBB/T+++9r69atWrhwodLS0hQWFiZJmjp1qj7++GPNnTtXR44c0cqVK7VhwwbNnj3b7CM9PV2///3vtWbNGn3wwQeaNm2aqqqqNGXKFL+cFwAAEHj8+kzTyZMnde+99+rzzz9X586ddeutt+qdd95R586dJUkvvfSSgoODNX78eFVXV8vlcmnlypXmz4eEhGjz5s2aNm2anE6n2rZtq5SUFD399NNmTVxcnHJycjR79mwtW7ZMXbt21auvviqXy2XWTJgwQadPn1ZGRobcbrcGDhyo3NzcBg+HAwCAHy6/rtPUkrBOU8vAOk0A8MPSrNZpAgAAaA4ITQAAABYQmgAAACwgNAEAAFhAaAIAALCA0AQAAGABoQkAAMACQhMAAIAFhCYAAAALCE0AAAAWEJoAAAAsIDQBAABYQGgCAACwgNAEAABgAaEJAADAAkITAACABYQmAAAACwhNAAAAFhCaAAAALCA0AQAAWEBoAgAAsIDQBAAAYAGhCQAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFhAaAIAALCA0AQAAGABoQkAAMACQhMAAIAFhCYAAAALCE0AAAAWEJoAAAAsIDQBAABYQGgCAACwoFGh6eOPP27qPgAAAAJao0JTz549NXz4cP35z3/WhQsXmronAACAgNOo0LR//371799f6enpio6O1n/+539q7969Td0bAABAwGhUaBo4cKCWLVum0tJS/fGPf1RZWZluvfVW9e3bVy+++KJOnz7d1H0CAAD41Xd6ELxVq1YaN26csrOz9dvf/lYfffSR5syZo9jYWE2ePFllZWWW53ruuecUFBSkWbNmmfsuXLigtLQ0dezYUe3atdP48eNVXl7u83MlJSVKTk5WmzZtFBkZqSeeeEIXL170qdm5c6cGDRqksLAw9ezZU1lZWQ2Ov2LFCvXo0UOtW7dWYmIid84AAICP7xSa9u3bp8cee0xdunTRiy++qDlz5ujYsWPatm2bSktLddddd1ma591339Xvfvc79e/f32f/7Nmz9eabbyo7O1v5+fkqLS3VuHHjzPG6ujolJyerpqZGu3fv1po1a5SVlaWMjAyz5vjx40pOTtbw4cNVVFSkWbNm6eGHH9bWrVvNmvXr1ys9PV2LFi3S/v37NWDAALlcLp06deq7nB4AANCCBBmGYVztD7344otavXq1iouLNWbMGD388MMaM2aMgoP/L4OdPHlSPXr0aHDX5+vOnTunQYMGaeXKlXrmmWc0cOBALV26VJWVlercubPWrl2re+65R5J05MgR3XTTTSooKNCwYcO0ZcsW3XHHHSotLVVUVJQkKTMzU/PmzdPp06dls9k0b9485eTk6NChQ+YxJ06cqIqKCuXm5kqSEhMTNWTIEC1fvlyS5PV6FRsbqxkzZmj+/PmWzonH45HD4VBlZaXsdrv1k4mA0mN+jr9b+EH45Llkf7cAAJKu7vd3o+40rVq1Svfdd58+/fRTbdq0SXfccYdPYJKkyMhI/eEPf7jiXGlpaUpOTlZSUpLP/sLCQtXW1vrs79Onj7p166aCggJJUkFBgfr162cGJklyuVzyeDw6fPiwWfP1uV0ulzlHTU2NCgsLfWqCg4OVlJRk1gAAALRqzA8dPXr0ijU2m00pKSnfWrNu3Trt379f7777boMxt9stm82mDh06+OyPioqS2+02ay4NTPXj9WPfVuPxeHT+/HmdOXNGdXV1l605cuTIN/ZeXV2t6upq87XH4/nW9woAAJq3Rt1pWr16tbKzsxvsz87O1po1ayzNceLECc2cOVOvvfaaWrdu3Zg2/Grx4sVyOBzmFhsb6++WAADANdSo0LR48WJ16tSpwf7IyEg9++yzluYoLCzUqVOnNGjQILVq1UqtWrVSfn6+Xn75ZbVq1UpRUVGqqalRRUWFz8+Vl5crOjpakhQdHd3g23T1r69UY7fbFR4erk6dOikkJOSyNfVzXM6CBQtUWVlpbidOnLD0vgEAQPPUqNBUUlKiuLi4Bvu7d++ukpISS3OMGDFCBw8eVFFRkbkNHjxYkyZNMv8dGhqqvLw882eKi4tVUlIip9MpSXI6nTp48KDPt9y2bdsmu92u+Ph4s+bSOepr6uew2WxKSEjwqfF6vcrLyzNrLicsLEx2u91nAwAALVejnmmKjIzUgQMH1KNHD5/977//vjp27Ghpjvbt26tv374++9q2bauOHTua+1NTU5Wenq6IiAjZ7XbNmDFDTqdTw4YNkySNHDlS8fHxeuCBB7RkyRK53W4tXLhQaWlpCgsLkyRNnTpVy5cv19y5c/XQQw9p+/bt2rBhg3Jy/u9bUunp6UpJSdHgwYM1dOhQLV26VFVVVZoyZUpjTg8AAGiBGhWa7r33Xv3yl79U+/btdfvtt0uS8vPzNXPmTE2cOLHJmnvppZcUHBys8ePHq7q6Wi6XSytXrjTHQ0JCtHnzZk2bNk1Op1Nt27ZVSkqKnn76abMmLi5OOTk5mj17tpYtW6auXbvq1VdflcvlMmsmTJig06dPKyMjQ263WwMHDlRubm6Dh8MBAMAPV6PWaaqpqdEDDzyg7OxstWr1Ve7yer2aPHmyMjMzZbPZmrzRQMc6TS0D6zR9P1inCUCguJrf342602Sz2bR+/Xr913/9l95//32Fh4erX79+6t69e6MaBgAACHSNCk31brzxRt14441N1QsAAEDAalRoqqurU1ZWlvLy8nTq1Cl5vV6f8e3btzdJcwAAAIGiUaFp5syZysrKUnJysvr27augoKCm7gsAACCgNCo0rVu3Ths2bNCYMWOauh8AAICA1KjFLW02m3r27NnUvQAAAASsRoWmxx9/XMuWLVMjVisAAABolhr18dy//vUv7dixQ1u2bNHNN9+s0NBQn/HXX3+9SZoDAAAIFI0KTR06dNDdd9/d1L0AAAAErEaFptWrVzd1HwAAAAGtUc80SdLFixf1j3/8Q7/73e909uxZSVJpaanOnTvXZM0BAAAEikbdafr00081atQolZSUqLq6Wj//+c/Vvn17/fa3v1V1dbUyMzObuk8AAAC/atSdppkzZ2rw4ME6c+aMwsPDzf1333238vLymqw5AACAQNGoO03//Oc/tXv3btlsNp/9PXr00L///e8maQwAACCQNOpOk9frVV1dXYP9J0+eVPv27b9zUwAAAIGmUaFp5MiRWrp0qfk6KChI586d06JFi/jTKgAAoEVq1MdzL7zwglwul+Lj43XhwgXdd999Onr0qDp16qS//OUvTd0jAACA3zUqNHXt2lXvv/++1q1bpwMHDujcuXNKTU3VpEmTfB4MBwAAaCkaFZokqVWrVrr//vubshcAAICA1ajQ9Kc//elbxydPntyoZgAAAAJVo0LTzJkzfV7X1tbqyy+/lM1mU5s2bQhNAACgxWnUt+fOnDnjs507d07FxcW69dZbeRAcAAC0SI3+23Nf16tXLz333HMN7kIBAAC0BE0WmqSvHg4vLS1tyikBAAACQqOeaXrjjTd8XhuGobKyMi1fvly33HJLkzQGAAAQSBoVmsaOHevzOigoSJ07d9bPfvYzvfDCC03RFwAAQEBpVGjyer1N3QcAAEBAa9JnmgAAAFqqRt1pSk9Pt1z74osvNuYQAAAAAaVRoem9997Te++9p9raWvXu3VuS9OGHHyokJESDBg0y64KCgpqmSwAAAD9rVGi688471b59e61Zs0bXXXedpK8WvJwyZYpuu+02Pf74403aJAAAgL8FGYZhXO0P/ehHP9Jbb72lm2++2Wf/oUOHNHLkyB/kWk0ej0cOh0OVlZWy2+3+bgeN1GN+jr9bQID65Llkf7cA4Bq4mt/fjXoQ3OPx6PTp0w32nz59WmfPnm3MlAAAAAGtUaHp7rvv1pQpU/T666/r5MmTOnnypP7nf/5HqampGjduXFP3CAAA4HeNeqYpMzNTc+bM0X333afa2tqvJmrVSqmpqXr++eebtEEAAIBA0KjQ1KZNG61cuVLPP/+8jh07Jkm64YYb1LZt2yZtDgAAIFB8p8Uty8rKVFZWpl69eqlt27ZqxDPlAAAAzUKjQtPnn3+uESNG6MYbb9SYMWNUVlYmSUpNTWW5AQAA0CI1KjTNnj1boaGhKikpUZs2bcz9EyZMUG5ubpM1BwAAECga9UzTW2+9pa1bt6pr164++3v16qVPP/20SRoDAAAIJI2601RVVeVzh6neF198obCwsO/cFAAAQKBpVGi67bbb9Kc//cl8HRQUJK/XqyVLlmj48OFN1hwAAECgaFRoWrJkiV555RWNHj1aNTU1mjt3rvr27atdu3bpt7/9reV5Vq1apf79+8tut8tut8vpdGrLli3m+IULF5SWlqaOHTuqXbt2Gj9+vMrLy33mKCkpUXJystq0aaPIyEg98cQTunjxok/Nzp07NWjQIIWFhalnz57Kyspq0MuKFSvUo0cPtW7dWomJidq7d+/VnRQAANCiNSo09e3bVx9++KFuvfVW3XXXXaqqqtK4ceP03nvv6YYbbrA8T9euXfXcc8+psLBQ+/bt089+9jPdddddOnz4sKSvHjh/8803lZ2drfz8fJWWlvqsOF5XV6fk5GTV1NRo9+7dWrNmjbKyspSRkWHWHD9+XMnJyRo+fLiKioo0a9YsPfzww9q6datZs379eqWnp2vRokXav3+/BgwYIJfLpVOnTjXm9AAAgBboqv9gb21trUaNGqXMzEz16tWryRuKiIjQ888/r3vuuUedO3fW2rVrdc8990iSjhw5optuukkFBQUaNmyYtmzZojvuuEOlpaWKioqS9NVq5fPmzdPp06dls9k0b9485eTk6NChQ+YxJk6cqIqKCvObfomJiRoyZIiWL18uSfJ6vYqNjdWMGTM0f/58S33zB3tbBv5gL74Jf7AXaJmu6R/sDQ0N1YEDBxrd3Depq6vTunXrVFVVJafTqcLCQtXW1iopKcms6dOnj7p166aCggJJUkFBgfr162cGJklyuVzyeDzm3aqCggKfOepr6ueoqalRYWGhT01wcLCSkpLMmsuprq6Wx+Px2QAAQMvVqI/n7r//fv3hD39okgYOHjyodu3aKSwsTFOnTtXGjRsVHx8vt9stm82mDh06+NRHRUXJ7XZLktxut09gqh+vH/u2Go/Ho/Pnz+uzzz5TXV3dZWvq57icxYsXy+FwmFtsbGyj3j8AAGgeGrVO08WLF/XHP/5R//jHP5SQkNDgb869+OKLlufq3bu3ioqKVFlZqb/+9a9KSUlRfn5+Y9r6Xi1YsEDp6enma4/HQ3ACAKAFu6rQ9PHHH6tHjx46dOiQBg0aJEn68MMPfWqCgoKuqgGbzaaePXtKkhISEvTuu+9q2bJlmjBhgmpqalRRUeFzt6m8vFzR0dGSpOjo6Abfcqv/dt2lNV//xl15ebnsdrvCw8MVEhKikJCQy9bUz3E5YWFhrEkFAMAPyFV9PNerVy999tln2rFjh3bs2KHIyEitW7fOfL1jxw5t3779OzXk9XpVXV2thIQEhYaGKi8vzxwrLi5WSUmJnE6nJMnpdOrgwYM+33Lbtm2b7Ha74uPjzZpL56ivqZ/DZrMpISHBp8br9SovL8+sAQAAuKo7TV//ot2WLVtUVVXV6IMvWLBAo0ePVrdu3XT27FmtXbtWO3fu1NatW+VwOJSamqr09HRFRETIbrdrxowZcjqdGjZsmCRp5MiRio+P1wMPPKAlS5bI7XZr4cKFSktLM+8CTZ06VcuXL9fcuXP10EMPafv27dqwYYNycv7vW1Lp6elKSUnR4MGDNXToUC1dulRVVVWaMmVKo98bAABoWRr1TFO9q1ytoIFTp05p8uTJKisrk8PhUP/+/bV161b9/Oc/lyS99NJLCg4O1vjx41VdXS2Xy6WVK1eaPx8SEqLNmzdr2rRpcjqdatu2rVJSUvT000+bNXFxccrJydHs2bO1bNkyde3aVa+++qpcLpdZM2HCBJ0+fVoZGRlyu90aOHCgcnNzGzwcDgAAfriuap2mkJAQud1ude7cWZLUvn17HThwQHFxcdesweaCdZpaBtZpwjdhnSagZbqa399X/fHcgw8+aH70deHCBU2dOrXBt+def/31q2wZAAAgsF1VaEpJSfF5ff/99zdpMwAAAIHqqkLT6tWrr1UfAAAAAa1RK4IDAAD80BCaAAAALCA0AQAAWEBoAgAAsIDQBAAAYAGhCQAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFhAaAIAALCA0AQAAGABoQkAAMACQhMAAIAFhCYAAAALCE0AAAAWEJoAAAAsIDQBAABYQGgCAACwgNAEAABgAaEJAADAAkITAACABYQmAAAACwhNAAAAFhCaAAAALGjl7wbQcvWYn+PvFgAAaDLcaQIAALCA0AQAAGABoQkAAMACQhMAAIAFhCYAAAALCE0AAAAWEJoAAAAsIDQBAABYQGgCAACwgNAEAABgAaEJAADAAr+GpsWLF2vIkCFq3769IiMjNXbsWBUXF/vUXLhwQWlpaerYsaPatWun8ePHq7y83KempKREycnJatOmjSIjI/XEE0/o4sWLPjU7d+7UoEGDFBYWpp49eyorK6tBPytWrFCPHj3UunVrJSYmau/evU3+ngEAQPPk19CUn5+vtLQ0vfPOO9q2bZtqa2s1cuRIVVVVmTWzZ8/Wm2++qezsbOXn56u0tFTjxo0zx+vq6pScnKyamhrt3r1ba9asUVZWljIyMsya48ePKzk5WcOHD1dRUZFmzZqlhx9+WFu3bjVr1q9fr/T0dC1atEj79+/XgAED5HK5dOrUqe/nZAAAgIAWZBiG4e8m6p0+fVqRkZHKz8/X7bffrsrKSnXu3Flr167VPffcI0k6cuSIbrrpJhUUFGjYsGHasmWL7rjjDpWWlioqKkqSlJmZqXnz5un06dOy2WyaN2+ecnJydOjQIfNYEydOVEVFhXJzcyVJiYmJGjJkiJYvXy5J8nq9io2N1YwZMzR//vwr9u7xeORwOFRZWSm73d7Up6ZZ6jE/x98tAE3mk+eS/d0CgGvgan5/B9QzTZWVlZKkiIgISVJhYaFqa2uVlJRk1vTp00fdunVTQUGBJKmgoED9+vUzA5MkuVwueTweHT582Ky5dI76mvo5ampqVFhY6FMTHByspKQks+brqqur5fF4fDYAANByBUxo8nq9mjVrlm655Rb17dtXkuR2u2Wz2dShQwef2qioKLndbrPm0sBUP14/9m01Ho9H58+f12effaa6urrL1tTP8XWLFy+Ww+Ewt9jY2Ma9cQAA0CwETGhKS0vToUOHtG7dOn+3YsmCBQtUWVlpbidOnPB3SwAA4Bpq5e8GJGn69OnavHmzdu3apa5du5r7o6OjVVNTo4qKCp+7TeXl5YqOjjZrvv4tt/pv111a8/Vv3JWXl8tutys8PFwhISEKCQm5bE39HF8XFhamsLCwxr1hAADQ7Pj1TpNhGJo+fbo2btyo7du3Ky4uzmc8ISFBoaGhysvLM/cVFxerpKRETqdTkuR0OnXw4EGfb7lt27ZNdrtd8fHxZs2lc9TX1M9hs9mUkJDgU+P1epWXl2fWAACAHza/3mlKS0vT2rVr9be//U3t27c3nx9yOBwKDw+Xw+FQamqq0tPTFRERIbvdrhkzZsjpdGrYsGGSpJEjRyo+Pl4PPPCAlixZIrfbrYULFyotLc28EzR16lQtX75cc+fO1UMPPaTt27drw4YNysn5v293paenKyUlRYMHD9bQoUO1dOlSVVVVacqUKd//iQEAAAHHr6Fp1apVkqSf/vSnPvtXr16tBx98UJL00ksvKTg4WOPHj1d1dbVcLpdWrlxp1oaEhGjz5s2aNm2anE6n2rZtq5SUFD399NNmTVxcnHJycjR79mwtW7ZMXbt21auvviqXy2XWTJgwQadPn1ZGRobcbrcGDhyo3NzcBg+HAwCAH6aAWqepOWOdpoZYpwktCes0AS1Ts12nCQAAIFARmgAAACwgNAEAAFhAaAIAALCA0AQAAGABoQkAAMACQhMAAIAFhCYAAAALCE0AAAAWEJoAAAAsIDQBAABYQGgCAACwgNAEAABgAaEJAADAAkITAACABYQmAAAACwhNAAAAFhCaAAAALCA0AQAAWEBoAgAAsIDQBAAAYAGhCQAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFhAaAIAALCA0AQAAGABoQkAAMACQhMAAIAFhCYAAAALCE0AAAAWEJoAAAAsIDQBAABYQGgCAACwgNAEAABgAaEJAADAAkITAACABYQmAAAAC1r5uwEAaA56zM/xdwtX7ZPnkv3dAtCi+PVO065du3TnnXcqJiZGQUFB2rRpk8+4YRjKyMhQly5dFB4erqSkJB09etSn5osvvtCkSZNkt9vVoUMHpaam6ty5cz41Bw4c0G233abWrVsrNjZWS5YsadBLdna2+vTpo9atW6tfv376+9//3uTvFwAANF9+DU1VVVUaMGCAVqxYcdnxJUuW6OWXX1ZmZqb27Nmjtm3byuVy6cKFC2bNpEmTdPjwYW3btk2bN2/Wrl279Oijj5rjHo9HI0eOVPfu3VVYWKjnn39eTz75pF555RWzZvfu3br33nuVmpqq9957T2PHjtXYsWN16NCha/fmAQBAsxJkGIbh7yYkKSgoSBs3btTYsWMlfXWXKSYmRo8//rjmzJkjSaqsrFRUVJSysrI0ceJEffDBB4qPj9e7776rwYMHS5Jyc3M1ZswYnTx5UjExMVq1apV+/etfy+12y2azSZLmz5+vTZs26ciRI5KkCRMmqKqqSps3bzb7GTZsmAYOHKjMzExL/Xs8HjkcDlVWVsputzfVaWnWmuPHGUBLwsdzwJVdze/vgH0Q/Pjx43K73UpKSjL3ORwOJSYmqqCgQJJUUFCgDh06mIFJkpKSkhQcHKw9e/aYNbfffrsZmCTJ5XKpuLhYZ86cMWsuPU59Tf1xLqe6uloej8dnAwAALVfAhia32y1JioqK8tkfFRVljrndbkVGRvqMt2rVShERET41l5vj0mN8U039+OUsXrxYDofD3GJjY6/2LQIAgGYkYENToFuwYIEqKyvN7cSJE/5uCQAAXEMBG5qio6MlSeXl5T77y8vLzbHo6GidOnXKZ/zixYv64osvfGouN8elx/immvrxywkLC5PdbvfZAABAyxWwoSkuLk7R0dHKy8sz93k8Hu3Zs0dOp1OS5HQ6VVFRocLCQrNm+/bt8nq9SkxMNGt27dql2tpas2bbtm3q3bu3rrvuOrPm0uPU19QfBwAAwK+h6dy5cyoqKlJRUZGkrx7+LioqUklJiYKCgjRr1iw988wzeuONN3Tw4EFNnjxZMTEx5jfsbrrpJo0aNUqPPPKI9u7dq7ffflvTp0/XxIkTFRMTI0m67777ZLPZlJqaqsOHD2v9+vVatmyZ0tPTzT5mzpyp3NxcvfDCCzpy5IiefPJJ7du3T9OnT/++TwkAAAhQfl0RfN++fRo+fLj5uj7IpKSkKCsrS3PnzlVVVZUeffRRVVRU6NZbb1Vubq5at25t/sxrr72m6dOna8SIEQoODtb48eP18ssvm+MOh0NvvfWW0tLSlJCQoE6dOikjI8NnLaef/OQnWrt2rRYuXKhf/epX6tWrlzZt2qS+fft+D2cBAAA0BwGzTlNzxzpNDbFOE+BfrNMEXFmLWKcJAAAgkBCaAAAALCA0AQAAWEBoAgAAsIDQBAAAYAGhCQAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFhAaAIAALCA0AQAAGABoQkAAMACQhMAAIAFhCYAAAALCE0AAAAWEJoAAAAsIDQBAABYQGgCAACwgNAEAABgAaEJAADAAkITAACABYQmAAAACwhNAAAAFhCaAAAALCA0AQAAWEBoAgAAsIDQBAAAYAGhCQAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFjQyt8NAACujR7zc/zdwlX75Llkf7cAfCPuNAEAAFhAaAIAALCAj+eaieZ4mx0AgJaE0AQACBjN8X8QeQ7rh4OP5wAAACwgNH3NihUr1KNHD7Vu3VqJiYnau3evv1sCAAABgNB0ifXr1ys9PV2LFi3S/v37NWDAALlcLp06dcrfrQEAAD8jNF3ixRdf1COPPKIpU6YoPj5emZmZatOmjf74xz/6uzUAAOBnPAj+v2pqalRYWKgFCxaY+4KDg5WUlKSCgoIG9dXV1aqurjZfV1ZWSpI8Hs816c9b/eU1mRcA8N10m53t7xYa5dBTLn+3EBDqf28bhnHFWkLT//rss89UV1enqKgon/1RUVE6cuRIg/rFixfrqaeearA/Njb2mvUIAEBTcSz1dweB5ezZs3I4HN9aQ2hqpAULFig9Pd187fV69cUXX6hjx44KCgryY2cNeTwexcbG6sSJE7Lb7f5up9njfDYdzmXT4nw2Hc5l0wrk82kYhs6ePauYmJgr1hKa/lenTp0UEhKi8vJyn/3l5eWKjo5uUB8WFqawsDCffR06dLiWLX5ndrs94C7W5ozz2XQ4l02L89l0OJdNK1DP55XuMNXjQfD/ZbPZlJCQoLy8PHOf1+tVXl6enE6nHzsDAACBgDtNl0hPT1dKSooGDx6soUOHaunSpaqqqtKUKVP83RoAAPAzQtMlJkyYoNOnTysjI0Nut1sDBw5Ubm5ug4fDm5uwsDAtWrSowceJaBzOZ9PhXDYtzmfT4Vw2rZZyPoMMK9+xAwAA+IHjmSYAAAALCE0AAAAWEJoAAAAsIDQBAABYQGhqwZ588kkFBQX5bH369PF3W83Grl27dOeddyomJkZBQUHatGmTz7hhGMrIyFCXLl0UHh6upKQkHT161D/NBrgrncsHH3ywwbU6atQo/zQb4BYvXqwhQ4aoffv2ioyM1NixY1VcXOxTc+HCBaWlpaljx45q166dxo8f32DhXnzFyvn86U9/2uD6nDp1qp86DlyrVq1S//79zQUsnU6ntmzZYo63hOuS0NTC3XzzzSorKzO3f/3rX/5uqdmoqqrSgAEDtGLFisuOL1myRC+//LIyMzO1Z88etW3bVi6XSxcuXPieOw18VzqXkjRq1Cifa/Uvf/nL99hh85Gfn6+0tDS988472rZtm2prazVy5EhVVVWZNbNnz9abb76p7Oxs5efnq7S0VOPGjfNj14HLyvmUpEceecTn+lyyZImfOg5cXbt21XPPPafCwkLt27dPP/vZz3TXXXfp8OHDklrIdWmgxVq0aJExYMAAf7fRIkgyNm7caL72er1GdHS08fzzz5v7KioqjLCwMOMvf/mLHzpsPr5+Lg3DMFJSUoy77rrLL/00d6dOnTIkGfn5+YZhfHUdhoaGGtnZ2WbNBx98YEgyCgoK/NVms/H182kYhvEf//EfxsyZM/3XVDN23XXXGa+++mqLuS6509TCHT16VDExMbr++us1adIklZSU+LulFuH48eNyu91KSkoy9zkcDiUmJqqgoMCPnTVfO3fuVGRkpHr37q1p06bp888/93dLzUJlZaUkKSIiQpJUWFio2tpan2uzT58+6tatG9emBV8/n/Vee+01derUSX379tWCBQv05Zdf+qO9ZqOurk7r1q1TVVWVnE5ni7kuWRG8BUtMTFRWVpZ69+6tsrIyPfXUU7rtttt06NAhtW/f3t/tNWtut1uSGqwWHxUVZY7BulGjRmncuHGKi4vTsWPH9Ktf/UqjR49WQUGBQkJC/N1ewPJ6vZo1a5ZuueUW9e3bV9JX16bNZmvwB8S5Nq/scudTku677z51795dMTExOnDggObNm6fi4mK9/vrrfuw2MB08eFBOp1MXLlxQu3bttHHjRsXHx6uoqKhFXJeEphZs9OjR5r/79++vxMREde/eXRs2bFBqaqofOwN8TZw40fx3v3791L9/f91www3auXOnRowY4cfOAltaWpoOHTrEs4pN5JvO56OPPmr+u1+/furSpYtGjBihY8eO6YYbbvi+2wxovXv3VlFRkSorK/XXv/5VKSkpys/P93dbTYaP535AOnTooBtvvFEfffSRv1tp9qKjoyWpwTc/ysvLzTE03vXXX69OnTpxrX6L6dOna/PmzdqxY4e6du1q7o+OjlZNTY0qKip86rk2v903nc/LSUxMlCSuz8uw2Wzq2bOnEhIStHjxYg0YMEDLli1rMdcloekH5Ny5czp27Ji6dOni71aavbi4OEVHRysvL8/c5/F4tGfPHjmdTj921jKcPHlSn3/+OdfqZRiGoenTp2vjxo3avn274uLifMYTEhIUGhrqc20WFxerpKSEa/MyrnQ+L6eoqEiSuD4t8Hq9qq6ubjHXJR/PtWBz5szRnXfeqe7du6u0tFSLFi1SSEiI7r33Xn+31iycO3fO5/8kjx8/rqKiIkVERKhbt26aNWuWnnnmGfXq1UtxcXH6zW9+o5iYGI0dO9Z/TQeobzuXEREReuqppzR+/HhFR0fr2LFjmjt3rnr27CmXy+XHrgNTWlqa1q5dq7/97W9q3769+TyIw+FQeHi4HA6HUlNTlZ6eroiICNntds2YMUNOp1PDhg3zc/eB50rn89ixY1q7dq3GjBmjjh076sCBA5o9e7Zuv/129e/f38/dB5YFCxZo9OjR6tatm86ePau1a9dq586d2rp1a8u5Lv399T1cOxMmTDC6dOli2Gw240c/+pExYcIE46OPPvJ3W83Gjh07DEkNtpSUFMMwvlp24De/+Y0RFRVlhIWFGSNGjDCKi4v923SA+rZz+eWXXxojR440OnfubISGhhrdu3c3HnnkEcPtdvu77YB0ufMoyVi9erVZc/78eeOxxx4zrrvuOqNNmzbG3XffbZSVlfmv6QB2pfNZUlJi3H777UZERIQRFhZm9OzZ03jiiSeMyspK/zYegB566CGje/fuhs1mMzp37myMGDHCeOutt8zxlnBdBhmGYXyfIQ0AAKA54pkmAAAACwhNAAAAFhCaAAAALCA0AQAAWEBoAgAAsIDQBAAAYAGhCQAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFjw/wGF5nNydc3NnwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df_20k.total_lines.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(12.789135747611642)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_20k.total_lines.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most of the content on each asbtract is shared between `methods` and `results` and that the average of total lines is 12.78, while most of them are allocated between 7 and 15 in a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create preprocess functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to turn this data into a data that our model can process and understand. For this purpose, we will use one hot encoding and encoded labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# One hot encode labels\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "train_labels_one_hot = one_hot_encoder.fit_transform(train_df_20k[\"type\"].to_numpy().reshape(-1, 1))\n",
    "val_labels_one_hot = one_hot_encoder.transform(val_df_20k[\"type\"].to_numpy().reshape(-1, 1))\n",
    "test_labels_one_hot = one_hot_encoder.transform(test_df_20k[\"type\"].to_numpy().reshape(-1, 1))\n",
    "\n",
    "# Check what training labels look like\n",
    "train_labels_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we transform the one-hot to label encoded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 2, 2, 2, 2, 4, 4, 4, 4])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode labels for train, validation, and test datasets\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode labels for training data\n",
    "train_labels_encoded = label_encoder.fit_transform(train_df_20k[\"type\"].to_numpy())\n",
    "val_labels_encoded = label_encoder.transform(val_df_20k[\"type\"].to_numpy())\n",
    "test_labels_encoded = label_encoder.transform(test_df_20k[\"type\"].to_numpy())\n",
    "\n",
    "# Check the encoded labels\n",
    "train_labels_encoded[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can have the direct assigment to the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " array(['BACKGROUND', 'CONCLUSIONS', 'METHODS', 'OBJECTIVE', 'RESULTS'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Get class names and number of classes from LabelEncoder instance \n",
    "num_classes = len(label_encoder.classes_)\n",
    "class_names = label_encoder.classes_\n",
    "num_classes, class_names\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also save the text to a string list as this is what we will fit into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert abstract text lines into lists \n",
    "train_sentences = train_df_20k[\"text\"].tolist()\n",
    "val_sentences = val_df_20k[\"text\"].tolist()\n",
    "test_sentences = test_df_20k[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For creating an embedding layer later on for dense models, we need to still prepare some part of the data. We checked what is the average number of sentences for minding what lenght we want to use in our model and if we want to use padding. But we also need to analyze the amount of words we have per sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(26.338269273494777)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "train_sentence_lengths = [len(sentence.split()) for sentence in train_sentences]\n",
    "avg_train_sentence_length = np.mean(train_sentence_lengths)\n",
    "avg_train_sentence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a view of how the data is distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(train_sentence_lengths, bins=7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to see how many tokens would need the 95% of our sentences..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# How long of a sentence covers 95% of the lengths?\n",
    "output_seq_len = int(np.percentile(train_sentence_lengths, 95))\n",
    "output_seq_len\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just to have an idea of what are some outliers here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(296, 1)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(train_sentence_lengths),min(train_sentence_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draft Model Experimentation\n",
    "\n",
    "We are ready to start doing our experiments as our data has been preprocessed. We will draft a plan as we have done before to know where will we start and how we will try to experiment and improve through different models.\n",
    "\n",
    " **Model iteration**      | **Model description**                                                                 |\n",
    "|--------------------------|---------------------------------------------------------------------------------|\n",
    "| **model_0**          | Model based on Naive Bayes |\n",
    "| **model_1**      |  NN with dense layers (dense model) |\n",
    "| **model_2**      | LSTM model  |\n",
    "| **model_3**      | GRU model  |\n",
    "| **model_4**      | Bidirectional LTSM model   |\n",
    "| **model_5**      | 1D Convolutional Neural Network   |\n",
    "| **model_6**      | Transfer learning model   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 0 : Baseline model based on Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline model will be a the TF-IDF Multinomial Naive Bayes model. As we did in our previous TF-NLP.ipynb, this is a simple model but that can be quite effective for simple task. Let's see how it performs here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a pipeline\n",
    "model_0 = Pipeline([\n",
    "  (\"tf-idf\", TfidfVectorizer()),\n",
    "  (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_0.fit(X=train_sentences, \n",
    "            y=train_labels_encoded);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already see how our model will classify the sentences from the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7218323844829869"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate baseline on validation dataset\n",
    "model_0.score(X=val_sentences,\n",
    "              y=val_labels_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a baseline model, it is quite a good results to achieve 72.18% accuracy. 7 out of 10 sentences will be categorized correctly from any abstract we read, and we can improve this results with more complex models in the future.\n",
    "\n",
    "To keep track of our progress, we can calculate the scores of this model and each model in the future and do comparisons to see what model will perform best for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 1, 3, ..., 4, 4, 1])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_preds = model_0.predict(val_sentences)\n",
    "baseline_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 72.1832384482987,\n",
       " 'precision': 0.7186466952323352,\n",
       " 'recall': 0.7218323844829869,\n",
       " 'f1': 0.6989250353450294}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_results = modelgraph.calculate_results(y_true=val_labels_encoded, \n",
    "                                    y_pred=baseline_preds)\n",
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Conv1d\n",
    "\n",
    "In order to build our first CNN for working on this NLP model, we need to start creating the first layers that will proccess our inputs (the text sentences) converting them into tokens and and add a group value for each sentence through our embedding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating text vectorizarion for the dense models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [PubMed200k RCT paper](https://arxiv.org/pdf/1710.06071), we can see that the PubMed20K raw data has 68.000 words and the 200k has 311.000 words. So for this model we can declare that maximum amount of tokens will be 68k for our [text vectorizer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization).\n",
    "\n",
    "As we checked before, the average sentence lenght is 26 but we will go for a 55 expected sentence lenght as the 95% of our data will have that lenght( and we don't want to lose valuable information from the abstracts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1740691032.413866    1947 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6096 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "max_tokens = 68000\n",
    "expected_sentence_lenght = 55\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens=max_tokens, # number of words in vocabulary\n",
    "                                    output_sequence_length=expected_sentence_lenght) # desired output length of vectorized sequences\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created our text_vectorizer, we need to pass it our text data so we have values assigned to our tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Adapt text vectorizer to training sentences\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look to an example and see how some sentence now has been converted to an specific token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "Planned interim analysis using the O'Brien-Fleming boundary was crossed for improvement in OS .\n",
      "\n",
      "Length of text: 14\n",
      "\n",
      "Vectorized text:\n",
      "[[ 1362  2426    85    59     2 29644 10060    10  3950    11   194     5\n",
      "    653     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test out text vectorizer\n",
    "import random\n",
    "target_sentence = random.choice(train_sentences)\n",
    "print(f\"Text:\\n{target_sentence}\")\n",
    "print(f\"\\nLength of text: {len(target_sentence.split())}\")\n",
    "print(f\"\\nVectorized text:\\n{text_vectorizer([target_sentence])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this sentence has the expected average lenght. As we still need to be wary of longer sentences, we have the expectetation of 55 words(tokens), so we count with padding to fill the unused spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check from our text vectorizer what are the most and least commont words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in vocabulary: 64841\n",
      "Most common words : ['', '[UNK]', 'the', 'and', 'of']\n",
      "Least common words : ['aainduced', 'aaigroup', 'aachener', 'aachen', 'aaacp']\n"
     ]
    }
   ],
   "source": [
    "rct_20k_vocab = text_vectorizer.get_vocabulary()\n",
    "rct_20k_vocab = [str(word) for word in rct_20k_vocab]\n",
    "print(f\"Words in vocabulary: {len(rct_20k_vocab)}\")\n",
    "print(f\"Most common words : {rct_20k_vocab[:5]}\")\n",
    "print(f\"Least common words : {rct_20k_vocab[-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'text_vectorization',\n",
       " 'trainable': True,\n",
       " 'dtype': {'module': 'keras',\n",
       "  'class_name': 'DTypePolicy',\n",
       "  'config': {'name': 'float32'},\n",
       "  'registered_name': None},\n",
       " 'max_tokens': 68000,\n",
       " 'standardize': 'lower_and_strip_punctuation',\n",
       " 'split': 'whitespace',\n",
       " 'ngrams': None,\n",
       " 'output_mode': 'int',\n",
       " 'output_sequence_length': 55,\n",
       " 'pad_to_max_tokens': False,\n",
       " 'sparse': False,\n",
       " 'ragged': False,\n",
       " 'vocabulary': None,\n",
       " 'idf_weights': None,\n",
       " 'encoding': 'utf-8',\n",
       " 'vocabulary_size': 64841}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorizer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the embedding layer for this specific model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we created our text vectorizer, we need to create the [embedding layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) so we can provide some relations between the numbers we are feeding into our model.\n",
    "\n",
    "As our model learns, the embedding layer will be updated and so, the relation concept that our embedding layer has between the tokens will evolve in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "# Create token embedding layer\n",
    "embedding_layer_0 = Embedding(input_dim=len(rct_20k_vocab), # length of vocabulary\n",
    "                               output_dim=128, # Note: different embedding sizes result in drastically different numbers of parameters to train\n",
    "                               # Use masking to handle variable sequence lengths (save space)\n",
    "                               mask_zero=True,\n",
    "                               name=\"embedding_layer_0\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to see an example : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence before vectorization:\n",
      "Reproducible associations were also observed for eSNPs in @ additional genes : fatty acid desaturase @ ( FADS@ ; P = @ ) , N-acetyl -- D-galactosaminidase ( NAGA ; P = @ ) , and Factor XIII , A@ ( F@A@ ; P = @ ) .\n",
      "\n",
      "Sentence after vectorization (before embedding):\n",
      "[[ 7791   992     9   138   148    11 23203     5   365  1847  1105   386\n",
      "  59920 31356    14  9367 59838 36658    14     3   432 41709     8  2723\n",
      "     14     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0]]\n",
      "\n",
      "Sentence after embedding:\n",
      "[[[-0.03645419  0.01443664  0.03296283 ... -0.02740195  0.01821331\n",
      "   -0.02045984]\n",
      "  [-0.00371559  0.0009904   0.00690639 ... -0.01459949  0.005814\n",
      "    0.02610642]\n",
      "  [-0.03514116 -0.01385161 -0.01242412 ...  0.0301263   0.02216018\n",
      "    0.03929644]\n",
      "  ...\n",
      "  [-0.02119676 -0.0453065  -0.01408998 ... -0.02054691 -0.00967202\n",
      "   -0.04784808]\n",
      "  [-0.02119676 -0.0453065  -0.01408998 ... -0.02054691 -0.00967202\n",
      "   -0.04784808]\n",
      "  [-0.02119676 -0.0453065  -0.01408998 ... -0.02054691 -0.00967202\n",
      "   -0.04784808]]]\n",
      "\n",
      "Embedded sentence shape: (1, 55, 128)\n"
     ]
    }
   ],
   "source": [
    "# Show example embedding\n",
    "print(f\"Sentence before vectorization:\\n{target_sentence}\\n\")\n",
    "vectorized_sentence = text_vectorizer([target_sentence])\n",
    "print(f\"Sentence after vectorization (before embedding):\\n{vectorized_sentence}\\n\")\n",
    "embedded_sentence = embedding_layer_0(vectorized_sentence)\n",
    "print(f\"Sentence after embedding:\\n{embedded_sentence}\\n\")\n",
    "print(f\"Embedded sentence shape: {embedded_sentence.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataset for our dense model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created our dataframes before with pandas and have organized our data. We can still make it easier for our model to process this data with the [tf.data API](https://www.tensorflow.org/guide/data). \n",
    "\n",
    "Choosing this method to prepare our data will take advantage of GPU usage and prepare the input pipeline [faster](https://www.tensorflow.org/guide/data_performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(5,), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn our data into TensorFlow Datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_one_hot))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_one_hot))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels_one_hot))\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our input processing layers created, our model is ready to be built.\n",
    "\n",
    "The structure we want to follow for the model is:\n",
    " 1. **Input** layer that will receive text.\n",
    " 2. **Tokenization** of the text.\n",
    " 3. **Embedding** process to stablish numerical relations.\n",
    " 4. **Grouped convoluted layers** to process and classify the relations.\n",
    " 5. **Ouput probability** for each sentence, determining the class of each sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's start creating now our Convoluted 1 Dimension model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/VSpython/tf-gpu-env/lib/python3.12/site-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'conv1d' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for weight initialization.\n",
    "tf.random.set_seed(10)\n",
    "\n",
    "# 1. Create the model.\n",
    "\n",
    "from tensorflow.keras.layers import Input,Conv1D,GlobalAveragePooling1D,Dense,LSTM\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# Create 1D convolutional model to process sequences\n",
    "inputs = Input(shape=(1,), dtype=tf.string)\n",
    "text_vectors = text_vectorizer(inputs) # vectorize text inputs\n",
    "token_embeddings = embedding_layer_0(text_vectors) # create embedding\n",
    "#x = LSTM(32, return_sequences=True)(token_embeddings) # condense the output of our feature vector\n",
    "x = Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(token_embeddings)\n",
    "x = GlobalAveragePooling1D()(x) # condense the output of our feature vector\n",
    "outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "model_1 = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ text_vectorization              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_layer_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">8,299,648</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">325</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ text_vectorization              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m55\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTextVectorization\u001b[0m)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_layer_0 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m55\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │     \u001b[38;5;34m8,299,648\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m55\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m41,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │           \u001b[38;5;34m325\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,340,997</span> (31.82 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,340,997\u001b[0m (31.82 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,340,997</span> (31.82 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,340,997\u001b[0m (31.82 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have quite a big amount of training parameters for our embedding layer if we take a look at the summary of our first convolution model.\n",
    "\n",
    "As we have also a large dataset (15k training samples, 2.5k of test and 2.5k of validation samples), so just to have an idea of how good the model could perform, we can use a 10% of the dataset to see how good the performance can be, we will take just a sub-set from it.\n",
    "\n",
    "It is a good practice to test the model on a small dataset and scale up as we progress in our experiments. 10% of the data is good, as long as we cover a good distribution of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Compile the model\n",
    "model_1.compile(loss=\"categorical_crossentropy\", # if your labels are integer form (not one hot) use sparse_categorical_crossentropy\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1740663002.636247    4606 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 20ms/step - accuracy: 0.5106 - loss: 1.1782 - val_accuracy: 0.7354 - val_loss: 0.6900\n",
      "Epoch 2/3\n",
      "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - accuracy: 0.7439 - loss: 0.6902 - val_accuracy: 0.7653 - val_loss: 0.6418\n",
      "Epoch 3/3\n",
      "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.7666 - loss: 0.6283 - val_accuracy: 0.7829 - val_loss: 0.6037\n"
     ]
    }
   ],
   "source": [
    "# 3. Fit the model\n",
    "model_1_history = model_1.fit(train_dataset,\n",
    "                              steps_per_epoch=int(0.1 * len(train_dataset)), # only fit on 10% of batches for faster training time\n",
    "                              epochs=3,\n",
    "                              validation_data=valid_dataset,\n",
    "                              validation_steps=int(0.1 * len(valid_dataset))) # only validate on 10% of batches\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model didn't take too long in the end and we have achieved a good accuracy. Let's make some predictions and see what score we achieve with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m945/945\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - accuracy: 0.7831 - loss: 0.6028\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6050218939781189, 0.7823381423950195]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on whole validation dataset (we only validated on 10% of batches during training)\n",
    "model_1.evaluate(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m945/945\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.1262600e-01, 1.8684800e-01, 9.0394348e-02, 2.7791914e-01,\n",
       "        3.2212533e-02],\n",
       "       [4.2741761e-01, 3.1738889e-01, 1.0916462e-02, 2.3580822e-01,\n",
       "        8.4688431e-03],\n",
       "       [1.9138981e-01, 7.3439963e-03, 2.7835551e-03, 7.9845357e-01,\n",
       "        2.9113093e-05],\n",
       "       ...,\n",
       "       [6.0392158e-06, 9.0391317e-04, 7.7856408e-04, 3.3982094e-06,\n",
       "        9.9830806e-01],\n",
       "       [3.7093561e-02, 4.4656178e-01, 8.0750965e-02, 4.0239897e-02,\n",
       "        3.9535385e-01],\n",
       "       [1.5520661e-01, 6.4159220e-01, 6.0008924e-02, 5.3801619e-02,\n",
       "        8.9390531e-02]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions (our model outputs prediction probabilities for each class)\n",
    "model_1_pred_probs = model_1.predict(valid_dataset)\n",
    "model_1_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 0, 3, ..., 4, 1, 1])>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert pred probs to classes\n",
    "model_1_preds = tf.argmax(model_1_pred_probs, axis=1)\n",
    "model_1_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 78.2338143783927,\n",
       " 'precision': 0.7782297800149067,\n",
       " 'recall': 0.782338143783927,\n",
       " 'f1': 0.7792336725259496}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Calculate model_1 results\n",
    "model_1_results = modelgraph.calculate_results(y_true=val_labels_encoded,\n",
    "                                    y_pred=model_1_preds)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 72.18, New accuracy: 78.23, Difference: 6.05\n",
      "Baseline precision: 0.72, New precision: 0.78, Difference: 0.06\n",
      "Baseline recall: 0.72, New recall: 0.78, Difference: 0.06\n",
      "Baseline f1: 0.70, New f1: 0.78, Difference: 0.08\n"
     ]
    }
   ],
   "source": [
    "modelgraph.CompareModelScores(baseline_results, model_1_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's jump into the next model now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 : Model 1, adding a LSTM layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model doesn't need much explanation. We will use a LSTM layer as it is recommended to save some information from the embedding layer before passing it into the convolutional 1D layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create token embedding layer\n",
    "embedding_layer_1 = Embedding(input_dim=len(rct_20k_vocab), # length of vocabulary\n",
    "                               output_dim=128, # Note: different embedding sizes result in drastically different numbers of parameters to train\n",
    "                               # Use masking to handle variable sequence lengths (save space)\n",
    "                               mask_zero=False,\n",
    "                               name=\"embedding_layer_1\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for weight initialization.\n",
    "tf.random.set_seed(10)\n",
    "\n",
    "# 1. Create the model.\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "# Create 1D convolutional model to process sequences\n",
    "inputs = Input(shape=(1,), dtype=tf.string)\n",
    "text_vectors = text_vectorizer(inputs) # vectorize text inputs\n",
    "token_embeddings = embedding_layer_1(text_vectors) # create embedding\n",
    "x = Bidirectional(LSTM(32, return_sequences=True))(token_embeddings)  # CuDNN-enabledx = Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(x)\n",
    "x = GlobalAveragePooling1D()(x) # condense the output of our feature vector\n",
    "outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "model_2 = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ text_vectorization              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">8,299,648</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,216</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_6      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">325</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_6 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ text_vectorization              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m55\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTextVectorization\u001b[0m)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_layer_1 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m55\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │     \u001b[38;5;34m8,299,648\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m55\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m41,216\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_6      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │           \u001b[38;5;34m325\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,341,189</span> (31.82 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,341,189\u001b[0m (31.82 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,341,189</span> (31.82 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,341,189\u001b[0m (31.82 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 47ms/step - accuracy: 0.5131 - loss: 1.1674 - val_accuracy: 0.7031 - val_loss: 0.7564\n",
      "Epoch 2/3\n",
      "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 37ms/step - accuracy: 0.7107 - loss: 0.7613 - val_accuracy: 0.7340 - val_loss: 0.6935\n",
      "Epoch 3/3\n",
      "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 48ms/step - accuracy: 0.7293 - loss: 0.6910 - val_accuracy: 0.7613 - val_loss: 0.6347\n"
     ]
    }
   ],
   "source": [
    "# 2. Compile the model\n",
    "model_2.compile(loss=\"categorical_crossentropy\", # if your labels are integer form (not one hot) use sparse_categorical_crossentropy\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# 3. Fit the model\n",
    "model_2_history = model_2.fit(train_dataset,\n",
    "                              steps_per_epoch=int(0.1 * len(train_dataset)), # only fit on 10% of batches for faster training time\n",
    "                              epochs=3,\n",
    "                              validation_data=valid_dataset,\n",
    "                              validation_steps=int(0.1 * len(valid_dataset))) # only validate on 10% of batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/945\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 35ms/step"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m945/945\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 76.64504170528267,\n",
       " 'precision': 0.7650154308829152,\n",
       " 'recall': 0.7664504170528267,\n",
       " 'f1': 0.763943424945686}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions (our model outputs prediction probabilities for each class)\n",
    "model_2_pred_probs = model_2.predict(valid_dataset)\n",
    "\n",
    "# Convert pred probs to classes\n",
    "model_2_preds = tf.argmax(model_2_pred_probs, axis=1)\n",
    "model_2_preds\n",
    "\n",
    "\n",
    "# Calculate model_2\n",
    "model_2_results = modelgraph.calculate_results(y_true=val_labels_encoded,\n",
    "                                    y_pred=model_2_preds)\n",
    "model_2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 78.23, New accuracy: 76.65, Difference: -1.59\n",
      "Baseline precision: 0.78, New precision: 0.77, Difference: -0.01\n",
      "Baseline recall: 0.78, New recall: 0.77, Difference: -0.02\n",
      "Baseline f1: 0.78, New f1: 0.76, Difference: -0.02\n"
     ]
    }
   ],
   "source": [
    "modelgraph.CompareModelScores(model_1_results,model_2_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems adding a Bidirectional LSTM did not have a great impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 : feature extraction layer\n",
    "\n",
    "For this model, we are going to use a model that replicates the architecture used in [Neural Netowrks for Joint Sentence classification](https://arxiv.org/pdf/1612.05251), which is the [GloVe](https://nlp.stanford.edu/projects/glove/) (Global Vectors for Word Representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our experiment, we can try to use transfer-learning model as a way of emulating this paper's approach. \n",
    "\n",
    "We will instead use a pretrain embedding (Universal Sentence Encoder) and then build our model on top of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Version: 2.18.0\n",
      "TF Hub version 0.16.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "print(\"TF Version: \" + tf.__version__)\n",
    "print(\"TF Hub version \" + hub.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_hub_embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        trainable=False,\n",
    "                                        name=\"universal_sentence_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the path for the model we want.\n",
    "uni_encoder_path = \"https://www.kaggle.com/models/google/universal-sentence-encoder/TensorFlow2/universal-sentence-encoder/2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1740671029.045769   11957 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6096 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Load the path for the model we want.\n",
    "uni_encoder_path = \"https://www.kaggle.com/models/google/universal-sentence-encoder/TensorFlow2/universal-sentence-encoder/2\"\n",
    "embed = hub.load(uni_encoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random training sentence:\n",
      "Furthermore , the patients were measured concerning weight and waist and hip circumference .\n",
      "\n",
      "Sentence after embedding:\n",
      "[-0.02885863  0.05677564 -0.04198207  0.02804436  0.02845938 -0.0440783\n",
      " -0.0575275   0.00180711 -0.01355083 -0.01458261  0.06209934 -0.01638985\n",
      " -0.0483803   0.06436092 -0.06140358 -0.00061076 -0.07922433  0.02606871\n",
      " -0.0662067   0.03277857  0.01203264  0.06532329  0.04292666  0.00482687\n",
      "  0.05076363  0.03274849  0.05714659  0.01979816  0.03962671  0.0129372 ] (truncated output)...\n",
      "\n",
      "Length of sentence embedding:\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Test out the embedding on a random sentence\n",
    "random_training_sentence = random.choice(train_sentences)\n",
    "print(f\"Random training sentence:\\n{random_training_sentence}\\n\")\n",
    "use_embedded_sentence = embed([random_training_sentence])\n",
    "print(f\"Sentence after embedding:\\n{use_embedded_sentence[0][:30]} (truncated output)...\\n\")\n",
    "print(f\"Length of sentence embedding:\\n{len(use_embedded_sentence[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only issue we have when using Tensorflow_hub is that we need to ensure that the `hub.KerasLayer` is properly used within the Keras model. The error occurs because the `hub.KerasLayer` expects a TensorFlow tensor, not a Keras tensor. That is why in this case we wrap  the `hub.KerasLayer` in a custom Keras layer.\n",
    "\n",
    "The downside of following this procedure is that we can't see the full trainable parameters we are having this layer.\n",
    "\n",
    "This is due to having Tensorflow 2.18 installed that deals with Keras 3 models. This provokes some incompatiblities between keras and tensorflow. The problem of rolling back to 2.15 where these issues don't exist relay on the rest of the function from tensorflow that have been updated to improve better overall efficiency from this library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'uni_encoder_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tf_hub_embedding_layer \u001b[38;5;241m=\u001b[39m hub\u001b[38;5;241m.\u001b[39mKerasLayer(\u001b[43muni_encoder_path\u001b[49m,\n\u001b[1;32m      2\u001b[0m                                         trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      3\u001b[0m                                         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniversal_sentence_encoder\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'uni_encoder_path' is not defined"
     ]
    }
   ],
   "source": [
    "tf_hub_embedding_layer = hub.KerasLayer(uni_encoder_path,\n",
    "                                        trainable=False,\n",
    "                                        name=\"universal_sentence_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf_keras as keras\n",
    "import tensorflow as tf\n",
    "\n",
    "model_3 = keras.Sequential([\n",
    "    tf_hub_embedding_layer,                         \n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(5, activation=\"softmax\")\n",
    "], name=\"model_3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Version: 2.18.0\n",
      "TF keras version 2.18.0\n",
      "TF Keras Version: 3.8.0\n",
      "TF Hub version 0.16.1\n"
     ]
    }
   ],
   "source": [
    "import tf_keras as keras\n",
    "version_fn = getattr(tf.keras, 'version', None)\n",
    "print('TF Version: ' + tf.__version__)\n",
    "print('TF keras version ' + keras.__version__)\n",
    "print('TF Keras Version: ' + version_fn())\n",
    "print('TF Hub version ' + hub.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF keras version 2.18.0\n"
     ]
    }
   ],
   "source": [
    "print('TF keras version ' + keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m inputs \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m[], dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mstring)\n\u001b[1;32m     15\u001b[0m pretrained_embeddings \u001b[38;5;241m=\u001b[39m tf_hub_embedding_layer(inputs)\n\u001b[0;32m---> 16\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mDense(num_classes, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)(x)\n\u001b[1;32m     19\u001b[0m model_3 \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mModel(inputs, outputs,name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/VSpython/tf-gpu-env/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/lib/python3.12/random.py:336\u001b[0m, in \u001b[0;36mRandom.randint\u001b[0;34m(self, a, b)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrandint\u001b[39m(\u001b[38;5;28mself\u001b[39m, a, b):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return random integer in range [a, b], including both end points.\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/random.py:312\u001b[0m, in \u001b[0;36mRandom.randrange\u001b[0;34m(self, start, stop, step)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty range for randrange()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# Stop argument supplied.\u001b[39;00m\n\u001b[0;32m--> 312\u001b[0m istop \u001b[38;5;241m=\u001b[39m \u001b[43m_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m width \u001b[38;5;241m=\u001b[39m istop \u001b[38;5;241m-\u001b[39m istart\n\u001b[1;32m    314\u001b[0m istep \u001b[38;5;241m=\u001b[39m _index(step)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from tf_keras import layers\n",
    "import tf_keras as keras\n",
    "\n",
    "# Set random seed for weight initialization.\n",
    "keras.utils.set_random_seed(10)\n",
    "\n",
    "tf_hub_embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        trainable=False,\n",
    "                                        name=\"universal_sentence_encoder\")\n",
    "\n",
    "# 1.Create the model\n",
    "inputs = layers.Input(shape=[], dtype=tf.string)\n",
    "pretrained_embeddings = tf_hub_embedding_layer(inputs)\n",
    "x = layers.Dense(128, activation=\"relu\")(pretrained_embeddings)\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model_3 = keras.Model(inputs, outputs,name=\"model_3\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Compile the model\n",
    "model_3.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/VSpython/tf-gpu-env/lib/python3.12/site-packages/tf_keras/src/engine/training.py:3501\u001b[0m, in \u001b[0;36mModel.summary\u001b[0;34m(self, line_length, positions, print_fn, expand_nested, show_trainable, layer_range)\u001b[0m\n\u001b[1;32m   3470\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prints a string summary of the network.\u001b[39;00m\n\u001b[1;32m   3471\u001b[0m \n\u001b[1;32m   3472\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3498\u001b[0m \u001b[38;5;124;03m    ValueError: if `summary()` is called before the model is built.\u001b[39;00m\n\u001b[1;32m   3499\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[0;32m-> 3501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3502\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model has not yet been built. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuild the model first by calling `build()` or by calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe model on a batch of data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3505\u001b[0m     )\n\u001b[1;32m   3506\u001b[0m layer_utils\u001b[38;5;241m.\u001b[39mprint_summary(\n\u001b[1;32m   3507\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3508\u001b[0m     line_length\u001b[38;5;241m=\u001b[39mline_length,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3513\u001b[0m     layer_range\u001b[38;5;241m=\u001b[39mlayer_range,\n\u001b[1;32m   3514\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data."
     ]
    }
   ],
   "source": [
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/VSpython/tf-gpu-env/lib/python3.12/site-packages/tf_keras/src/initializers/initializers.py:121: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/home/david/VSpython/tf-gpu-env/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 1398, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/david/VSpython/tf-gpu-env/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 1381, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/david/VSpython/tf-gpu-env/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 1370, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/david/VSpython/tf-gpu-env/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 1147, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/david/VSpython/tf-gpu-env/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/lib/python3.12/random.py\", line 336, in randint\n        return self.randrange(a, b+1)\n    File \"/usr/lib/python3.12/random.py\", line 312, in randrange\n        istop = _index(stop)\n\n    TypeError: Exception encountered when calling layer 'model_3' (type Sequential).\n    \n    'float' object cannot be interpreted as an integer\n    \n    Call arguments received by layer 'model_3' (type Sequential):\n      • inputs=tf.Tensor(shape=(None,), dtype=string)\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_3_history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                              \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# only fit on 10% of batches for faster training time\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# only validate on 10% of batches\u001b[39;00m\n",
      "File \u001b[0;32m~/VSpython/tf-gpu-env/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filezuktrofb.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/random.py:336\u001b[0m, in \u001b[0;36mRandom.randint\u001b[0;34m(self, a, b)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrandint\u001b[39m(\u001b[38;5;28mself\u001b[39m, a, b):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return random integer in range [a, b], including both end points.\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/random.py:312\u001b[0m, in \u001b[0;36mRandom.randrange\u001b[0;34m(self, start, stop, step)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty range for randrange()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# Stop argument supplied.\u001b[39;00m\n\u001b[0;32m--> 312\u001b[0m istop \u001b[38;5;241m=\u001b[39m \u001b[43m_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m width \u001b[38;5;241m=\u001b[39m istop \u001b[38;5;241m-\u001b[39m istart\n\u001b[1;32m    314\u001b[0m istep \u001b[38;5;241m=\u001b[39m _index(step)\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/home/david/VSpython/tf-gpu-env/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 1398, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/david/VSpython/tf-gpu-env/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 1381, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/david/VSpython/tf-gpu-env/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 1370, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/david/VSpython/tf-gpu-env/lib/python3.12/site-packages/tf_keras/src/engine/training.py\", line 1147, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/david/VSpython/tf-gpu-env/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/lib/python3.12/random.py\", line 336, in randint\n        return self.randrange(a, b+1)\n    File \"/usr/lib/python3.12/random.py\", line 312, in randrange\n        istop = _index(stop)\n\n    TypeError: Exception encountered when calling layer 'model_3' (type Sequential).\n    \n    'float' object cannot be interpreted as an integer\n    \n    Call arguments received by layer 'model_3' (type Sequential):\n      • inputs=tf.Tensor(shape=(None,), dtype=string)\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "model_3_history = model_3.fit(train_dataset,\n",
    "                              steps_per_epoch=int(0.1 * len(train_dataset)), # only fit on 10% of batches for faster training time\n",
    "                              epochs=3,\n",
    "                              validation_data=valid_dataset,\n",
    "                              validation_steps=int(0.1 * len(valid_dataset))) # only validate on 10% of batches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
