{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/davalpez/MyKaggle/blob/MyKaggle/Tensorflow%20basics/TF-architecture.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In depth architecture and structure for basic Neural Networks\n",
    "\n",
    "In other notebooks of this section, we have taken a look at how neural networks help us with different regression and classification problems. I would like to go a bit in depth to see exactly in this structure we have built, to understand better how the networks evolve when we train it and comprehend better what goes behind each step of the training."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recalling from our TF-regression notebook, the structure of a basic NN could look like this :\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "As we can see, each layer has a certain amount of neurons that is fully interconnected with the neurons from previous and posterior layers.\n",
    "\n",
    "When in our notebooks we store a model we have created, we are saving this information about out model:\n",
    "\n",
    "* The weight values\n",
    "* The model's architecture\n",
    "* The model's training configuration (what you pass to the .compile() method)\n",
    "* The optimizer and its state, if any (this enables you to restart training where you left off)\n",
    "\n",
    "Let's go through, one by one, the information we save to understand better the work we are saving here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Model's architecture.\n",
    "\n",
    "It is not the first time we plot our model. One part of information we save into the keras file is the number of layers, output shape of each layer and the number of parameters on each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start first for bringing back some example that we have worked with. I would bring back the last model saved from regression, but to see better how the propagation works, we will recreate some data and create a model with just 3 layers and a few neurons to see how the process goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate some data from TF-regression for our experiment.\n",
    "import numpy as np\n",
    "\n",
    "def OurLinearCorrelation(x):\n",
    "    '''Function to create linear correlation on a dependant variable\n",
    "    '''\n",
    "    return ((2*x)+3)\n",
    "\n",
    "x_large = np.arange(-100,50,2)\n",
    "y_large = OurLinearCorrelation(x_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create batches for training and testing the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_large[:16]\n",
    "x_test = x_large[:16]\n",
    "y_train = y_large[16:]\n",
    "x_train = x_large[16:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look to what the data looks like. As we see large values, it is recommended to normalize these. We will try to first make the neural network work as we do in tensorflow, but keep in mind that networks work better with smaller values, so in the future we will always try to make these normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, -70, 99, -137)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.max(),x_test.max(),y_train.max(),y_test.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-68, -100, -133, -197)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.min(),x_test.min(),y_train.min(),y_test.min(),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now build a neural network as we have done before for regression with tensorflow, to see the architecture we aim to follow and the results we expect to have. First, we import the tensorflow library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 13:11:04.381790: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-27 13:11:04.417894: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-27 13:11:04.496568: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-27 13:11:04.582976: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-27 13:11:04.583191: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-27 13:11:04.670658: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-27 13:11:06.944168: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this excercise, I want to show how the weight and biases change overtime. This is possible to do on Tensorflow with callbacks. If we need to obtain any other information in Tensorflow while debuggin a new network, this is a good way to verify what is happening inside our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintWeightsCallback(tf.keras.callbacks.Callback):\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        print(f\"Batch {batch + 1}:\")\n",
    "        for layer in self.model.layers:\n",
    "            weights = layer.get_weights()  # Get weights and biases of the layer\n",
    "            if weights: \n",
    "                print(f\"Weights of {layer.name}: {weights[0]}\") \n",
    "                print(f\"Biases of {layer.name}: {weights[1]}\") \n",
    "                print(f\"output of layer of {layer.name}: {layer.output}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a simple neural network as we have done before. See how this time, we are using this callback function we created in the class PrintWeightCallback to see how the weights evolve overtime in our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 13:11:09.432164: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-27 13:11:09.432724: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "Weights of dense: [[ 0.5336266 -1.4189976]]\n",
      "Biases of dense: [-0.00139105  0.00149671]\n",
      "output of layer of dense: <KerasTensor shape=(None, 2), dtype=float32, sparse=False, name=keras_tensor_1>\n",
      "Weights of dense_1: [[-0.8687154  -0.73066896]\n",
      " [ 1.1269513   1.3052276 ]]\n",
      "Biases of dense_1: [0.00084217 0.00074181]\n",
      "output of layer of dense_1: <KerasTensor shape=(None, 2), dtype=float32, sparse=False, name=keras_tensor_2>\n",
      "Weights of dense_2: [[-1.4687964]\n",
      " [-1.3610137]]\n",
      "Biases of dense_2: [-0.000625]\n",
      "output of layer of dense_2: <KerasTensor shape=(None, 1), dtype=float32, sparse=False, name=keras_tensor_3>\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 602ms/step - loss: 27.0496 - mae: 27.0496Batch 2:\n",
      "Weights of dense: [[-0.10713542 -0.45049715]]\n",
      "Biases of dense: [ 0.00449521 -0.00740027]\n",
      "output of layer of dense: <KerasTensor shape=(None, 2), dtype=float32, sparse=False, name=keras_tensor_1>\n",
      "Weights of dense_1: [[-0.6475075  -0.52569366]\n",
      " [ 0.5387329   0.7601736 ]]\n",
      "Biases of dense_1: [-0.00296582 -0.00278674]\n",
      "output of layer of dense_1: <KerasTensor shape=(None, 2), dtype=float32, sparse=False, name=keras_tensor_2>\n",
      "Weights of dense_2: [[-0.8866439 ]\n",
      " [-0.72825634]]\n",
      "Biases of dense_2: [0.00196759]\n",
      "output of layer of dense_2: <KerasTensor shape=(None, 1), dtype=float32, sparse=False, name=keras_tensor_3>\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 54.1745 - mae: 54.1745 \n",
      "Epoch 2/5\n",
      "Batch 1:\n",
      "Weights of dense: [[ 0.20028421 -0.7817912 ]]\n",
      "Biases of dense: [ 0.00389712 -0.00675573]\n",
      "output of layer of dense: <KerasTensor shape=(None, 2), dtype=float32, sparse=False, name=keras_tensor_1>\n",
      "Weights of dense_1: [[-0.61698914 -0.50062704]\n",
      " [ 0.66704583  0.8655651 ]]\n",
      "Biases of dense_1: [-0.00241167 -0.00233158]\n",
      "output of layer of dense_1: <KerasTensor shape=(None, 2), dtype=float32, sparse=False, name=keras_tensor_2>\n",
      "Weights of dense_2: [[-0.9423189]\n",
      " [-0.8201707]]\n",
      "Biases of dense_2: [0.00134259]\n",
      "output of layer of dense_2: <KerasTensor shape=(None, 1), dtype=float32, sparse=False, name=keras_tensor_3>\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 52.4325 - mae: 52.4325Batch 2:\n",
      "Weights of dense: [[ 0.47877914 -1.1575574 ]]\n",
      "Biases of dense: [ 0.00206008 -0.00427706]\n",
      "output of layer of dense: <KerasTensor shape=(None, 2), dtype=float32, sparse=False, name=keras_tensor_1>\n",
      "Weights of dense_1: [[-0.669967  -0.5467376]\n",
      " [ 0.8738548  1.0455664]]\n",
      "Biases of dense_1: [-0.00066663 -0.00081275]\n",
      "output of layer of dense_1: <KerasTensor shape=(None, 2), dtype=float32, sparse=False, name=keras_tensor_2>\n",
      "Weights of dense_2: [[-1.1233974]\n",
      " [-1.038276 ]]\n",
      "Biases of dense_2: [-0.00050926]\n",
      "output of layer of dense_2: <KerasTensor shape=(None, 1), dtype=float32, sparse=False, name=keras_tensor_3>\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 42.7334 - mae: 42.7334\n",
      "Epoch 3/5\n",
      "Batch 1:\n",
      "Weights of dense: [[ 0.05628201 -0.49603003]]\n",
      "Biases of dense: [ 0.00453565 -0.0081532 ]\n",
      "output of layer of dense: <KerasTensor shape=(None, 2), dtype=float32, sparse=False, name=keras_tensor_1>\n",
      "Weights of dense_1: [[-0.4978564  -0.38766807]\n",
      " [ 0.45773682  0.6609783 ]]\n",
      "Biases of dense_1: [-0.002773   -0.00275951]\n",
      "output of layer of dense_1: <KerasTensor shape=(None, 2), dtype=float32, sparse=False, name=keras_tensor_2>\n",
      "Weights of dense_2: [[-0.6970711]\n",
      " [-0.5672257]]\n",
      "Biases of dense_2: [0.00136574]\n",
      "output of layer of dense_2: <KerasTensor shape=(None, 1), dtype=float32, sparse=False, name=keras_tensor_3>\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 33.3640 - mae: 33.3640Batch 2:\n",
      "Weights of dense: [[ 0.21628411 -0.69189197]]\n",
      "Biases of dense: [ 0.00306582 -0.00635394]\n",
      "output of layer of dense: <KerasTensor shape=(None, 2), dtype=float32, sparse=False, name=keras_tensor_1>\n",
      "Weights of dense_1: [[-0.5089205  -0.39667124]\n",
      " [ 0.55530554  0.74037266]]\n",
      "Biases of dense_1: [-0.00096578 -0.00128893]\n",
      "output of layer of dense_1: <KerasTensor shape=(None, 2), dtype=float32, sparse=False, name=keras_tensor_2>\n",
      "Weights of dense_2: [[-0.7690352 ]\n",
      " [-0.66588855]]\n",
      "Biases of dense_2: [-0.00122685]\n",
      "output of layer of dense_2: <KerasTensor shape=(None, 1), dtype=float32, sparse=False, name=keras_tensor_3>\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 36.9305 - mae: 36.9305 \n",
      "Epoch 4/5\n",
      "Batch 1:\n",
      "Weights of dense: [[ 0.42686883 -0.98745966]]\n",
      "Biases of dense: [ 0.00265612 -0.00577891]\n",
      "output of layer of dense: <KerasTensor shape=(None, 2), dtype=float32, sparse=False, name=keras_tensor_1>\n",
      "Weights of dense_1: [[-0.56235254 -0.44293675]\n",
      " [ 0.72623616  0.88837725]]\n",
      "Biases of dense_1: [-0.00048513 -0.00087275]\n",
      "output of layer of dense_1: <KerasTensor shape=(None, 2), dtype=float32, sparse=False, name=keras_tensor_2>\n",
      "Weights of dense_2: [[-0.9278198]\n",
      " [-0.8580081]]\n",
      "Biases of dense_2: [-0.00185185]\n",
      "output of layer of dense_2: <KerasTensor shape=(None, 1), dtype=float32, sparse=False, name=keras_tensor_3>\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 39.0583 - mae: 39.0583Batch 2:\n",
      "Weights of dense: [[ 0.6626742 -1.3629603]]\n",
      "Biases of dense: [ 0.00299012 -0.00631078]\n",
      "output of layer of dense: <KerasTensor shape=(None, 2), dtype=float32, sparse=False, name=keras_tensor_1>\n",
      "Weights of dense_1: [[-0.66591513 -0.538707  ]\n",
      " [ 0.96580344  1.1099188 ]]\n",
      "Biases of dense_1: [-0.00082877 -0.00119053]\n",
      "output of layer of dense_1: <KerasTensor shape=(None, 2), dtype=float32, sparse=False, name=keras_tensor_2>\n",
      "Weights of dense_2: [[-1.1781068]\n",
      " [-1.1368318]]\n",
      "Biases of dense_2: [-0.00148148]\n",
      "output of layer of dense_2: <KerasTensor shape=(None, 1), dtype=float32, sparse=False, name=keras_tensor_3>\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 28.7476 - mae: 28.7476 \n",
      "Epoch 5/5\n",
      "Batch 1:\n",
      "Weights of dense: [[ 0.21390775 -0.5920855 ]]\n",
      "Biases of dense: [ 0.0047363  -0.00931029]\n",
      "output of layer of dense: <KerasTensor shape=(None, 2), dtype=float32, sparse=False, name=keras_tensor_1>\n",
      "Weights of dense_1: [[-0.41511935 -0.29669785]\n",
      " [ 0.44997752  0.61216486]]\n",
      "Biases of dense_1: [-0.0023014  -0.00261157]\n",
      "output of layer of dense_1: <KerasTensor shape=(None, 2), dtype=float32, sparse=False, name=keras_tensor_2>\n",
      "Weights of dense_2: [[-0.61347723]\n",
      " [-0.5361828 ]]\n",
      "Biases of dense_2: [-0.00023148]\n",
      "output of layer of dense_2: <KerasTensor shape=(None, 1), dtype=float32, sparse=False, name=keras_tensor_3>\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 70.9283 - mae: 70.9283Batch 2:\n",
      "Weights of dense: [[ 0.33067733 -0.7626276 ]]\n",
      "Biases of dense: [ 0.00366361 -0.00774363]\n",
      "output of layer of dense: <KerasTensor shape=(None, 2), dtype=float32, sparse=False, name=keras_tensor_1>\n",
      "Weights of dense_1: [[-0.45214713 -0.32906038]\n",
      " [ 0.55247456  0.7017479 ]]\n",
      "Biases of dense_1: [-0.00071091 -0.00122146]\n",
      "output of layer of dense_1: <KerasTensor shape=(None, 2), dtype=float32, sparse=False, name=keras_tensor_2>\n",
      "Weights of dense_2: [[-0.713707 ]\n",
      " [-0.6563617]]\n",
      "Biases of dense_2: [-0.00282407]\n",
      "output of layer of dense_2: <KerasTensor shape=(None, 1), dtype=float32, sparse=False, name=keras_tensor_3>\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 62.4303 - mae: 62.4303 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fc7c25f5d30>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple NN for regression\n",
    "\n",
    "# Set random seed for weight initialization.\n",
    "tf.random.set_seed(10)\n",
    "\n",
    "# 1. Create our model\n",
    "\n",
    "model_1 = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(2),\n",
    "  tf.keras.layers.Dense(2),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "\n",
    "model_1.compile(loss=tf.keras.losses.mae,\n",
    "              optimizer=tf.keras.optimizers.SGD(),\n",
    "              metrics=[\"mae\"])\n",
    "\n",
    "# 3. Fit the model\n",
    "\n",
    "model_1.fit(tf.expand_dims(x_train, axis=-1), y_train, epochs=5,verbose=1,callbacks=[PrintWeightsCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this are the layers and the parameters we have in our simple neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │             \u001b[38;5;34m4\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │             \u001b[38;5;34m6\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m3\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15</span> (64.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m15\u001b[0m (64.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13</span> (52.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m13\u001b[0m (52.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the shapes of the network\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parameters shown in our summary corresond to the weights and biases from our model.\n",
    "\n",
    "* In the first dense layer, we have 2 weights + 2 biases.\n",
    "\n",
    "* In the second layer, we have 2 weights from the previous layer multiplied by the 2 weights from that second layer, plus 2 biases from the second layer, making it 6 parameters.\n",
    "\n",
    "* In the third layer, it is 2 weights from the previous layer multiplied by only 1 weight from that output layer, plus one bias from that output layer, making it 3 parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer parameters are parameters created for the optimizer. In this case, we are using the [SGD optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD). These two parameters correspond to `learning rate` and the other one for a `step counter` maintained by the optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight values with Forward and Back Propagation\n",
    "\n",
    "As we explained in \"the basics\" notebook, each layer is formed by neurons. Each of these neurons are going to have weights, set as random values. We set the random seed at the beginning to ensure that these weights are still randomized but makes it easier for us to reproduce our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed weights:\n",
      "tf.Tensor(\n",
      "[[ 0.561846   -0.2693448  -0.0558188 ]\n",
      " [ 0.61675906 -0.9262366   0.6609192 ]\n",
      " [ 0.42180896 -0.9863157   0.13641763]], shape=(3, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.561846   -0.2693448  -0.0558188 ]\n",
      " [ 0.61675906 -0.9262366   0.6609192 ]\n",
      " [ 0.42180896 -0.9863157   0.13641763]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(10)  # Set seed for reproducibility\n",
    "initializer = tf.keras.initializers.GlorotUniform()\n",
    "weights_1 = initializer(shape=(3, 3))\n",
    "\n",
    "tf.random.set_seed(10)  # Reset seed\n",
    "weights_2 = initializer(shape=(3, 3))\n",
    "\n",
    "print(\"Random seed weights:\")\n",
    "print(weights_1)\n",
    "print(weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These neurons output consist on the formula:\n",
    "\n",
    "$z=W⋅x+b$\n",
    "Where :\n",
    "\n",
    "* $W$: Weight vector for the neuron.\n",
    "* $x$: Input vector from the previous layer ( $x=a^{prev}  f(z)$).\n",
    "* $b$: Bias for the neuron.\n",
    "* $z$: Weighted sum before applying activation.\n",
    "\n",
    "Our Z is passed through our activation function. If we don't specify anything when creating a layer, like in the some basic models we have built in our TF-regression notebook, we would have a passthrough function ( or also known as [linear function](https://www.tensorflow.org/api_docs/python/tf/keras/activations/linear)). In our multiclassification network, We have set [RelU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU).\n",
    "\n",
    "$a=f(z)$\n",
    "\n",
    "So, when we calculate the first predictions ( these probability matrix that will end up forming the probability array in the output layer), our NN is only computing this function of $z$ and passing values. And this is called the `Forward Pass` ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a prediction, the loss is calculated depending the function we are using ( mse or mae are some possible uses). Once we have the loss value, we continue doing the `Backwards pass` or Backproagation. \n",
    "\n",
    "In this step, the weights and the biases are adjusted after calculating the gradiant descends following the formula:\n",
    "\n",
    "\n",
    "$W ← w - η \\small\\frac{\\partial L}{\\partial W} $\n",
    "\n",
    "​Where:\n",
    "* W is the updated weight.\n",
    "* w is the previous weight.\n",
    "* η is the learning rate.\n",
    " \n",
    "* $ \\small\\frac{\\partial L}{\\partial W}$\n",
    "is the loss gradient with respect to the weight.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, for our biases would be \n",
    "\n",
    "$B ← b - η \\small\\frac{\\partial L}{\\partial b} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a glimpse of how our array of weights and biases look in the model we created in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All biases :  [array([ 0.00366361, -0.00774363], dtype=float32), array([-0.00071091, -0.00122146], dtype=float32), array([-0.00282407], dtype=float32)]\n",
      "All weights :  [array([[ 0.33067733, -0.7626276 ]], dtype=float32), array([[-0.45214713, -0.32906038],\n",
      "       [ 0.55247456,  0.7017479 ]], dtype=float32), array([[-0.713707 ],\n",
      "       [-0.6563617]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Create some list to store the arrays of weights and biases from each layer\n",
    "weights_list=[]\n",
    "biases_list=[]\n",
    "for layer in model_1.layers:\n",
    "    weights, biases = layer.get_weights()  # get_weights() returns [weights, biases]\n",
    "    weights_list.append(weights)\n",
    "    biases_list.append(biases)\n",
    "print(\"All biases : \",biases_list)\n",
    "print(\"All weights : \",weights_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imitating forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try reproducing this ourselves to see what goes on in this stage of the training. We will take just a piece of the data an emulate how would this data go when doing  the `forward pass` stage.\n",
    "\n",
    "For making it a bit more clear what happens with the activation function in other models as in multiclassification model, we will use [RelU](https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu) as the our activation for our layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first create a function to simulate the weight creation depending on the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.29604465, -0.21134205]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def LayerCreation(input_size=1,neurons=2):\n",
    "    '''\n",
    "    \n",
    "    Creates weight and biases values as we would have in a dense layer\n",
    "    in tensorflow.\n",
    "\n",
    "    Args:\n",
    "    input_size : size of the expected input.\n",
    "    neurons : amount of neurons in the layer.\n",
    "\n",
    "    return:\n",
    "    W_tensor : tensor with weights that compose the layer.\n",
    "    b_tensor : tensor with the biases that compose the layer.\n",
    "    '''\n",
    "    initializer = tf.keras.initializers.Zeros()\n",
    "    random_tensor_w1 = tf.random.Generator.from_seed(10) \n",
    "    W_tensor =  random_tensor_w1.normal(shape=(input_size,neurons))\n",
    "    random_tensor_b1 = tf.random.Generator.from_seed(11) \n",
    "    b_tensor =  initializer(shape=(neurons,))\n",
    "    return W_tensor,b_tensor\n",
    "\n",
    "LayerCreation(1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensorflow library already helps us with converting the inputs to tensors and to the dtypes we already need, but to show here step by step the evolution of the weights, we will also create a function to treat the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrepareInput(input):\n",
    "    '''\n",
    "    This function prepares the input array so it is ready to be introduced\n",
    "    to the input layer we are creating as a simulation of a neural network.\n",
    "    \n",
    "    Args:\n",
    "    input : input array we will feed to our first layer.\n",
    "\n",
    "    return:\n",
    "    prepared_input: tensor that is prepared to be introduced in the first layer.\n",
    "    '''\n",
    "    expanded_input = (tf.expand_dims(input, axis=-1))\n",
    "    prepared_input = tf.cast(expanded_input, dtype=tf.float32)\n",
    "    return prepared_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create what would be the 3 layer model for forward propagation but without recurring to the tensorflow models, to see closely how the weights and biases interact in the model with the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "input_example = x_train[50:]\n",
    "prepared_input = PrepareInput(input_example)\n",
    "\n",
    "# Layer 1\n",
    "\n",
    "W1tf,b1tf = LayerCreation(1,2)\n",
    "\n",
    "# Layer 2\n",
    "\n",
    "W2tf,b2tf = LayerCreation(2,2)\n",
    "\n",
    "# Layer 3\n",
    "\n",
    "W3tf,b3tf = LayerCreation(2,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1tf : tf.Tensor([[-0.29604465 -0.21134205]], shape=(1, 2), dtype=float32)\n",
      "b1tf : tf.Tensor([0. 0.], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"W1tf :\",W1tf)\n",
    "print(\"b1tf :\",b1tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have prepared the input data, and we have our layers defined with their own weights and biases, let's try to do the forward propagation for the first epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation stage\n",
    "\n",
    "def linear_activation(x):\n",
    "    '''Linear activation that returns the same value.\n",
    "    '''\n",
    "    return (x)\n",
    "\n",
    "def relu_activation(x):\n",
    "    '''Relu activation that returns the element-wise maximum for 0 and the input value.\n",
    "    '''\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    '''Relu derivative that returns 1 if the input is greater than 0 or returns 0 otherwise.\n",
    "    '''\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def linear_derivative(x):\n",
    "    '''Linear derivative returns one, as the derivative of x is equal to 1 always.\"\n",
    "    '''\n",
    "    return np.ones_like(x)\n",
    "\n",
    "def forward_propagation(input_value,W,b,activation_function=linear_activation):\n",
    "    ''' Calculates the activation values and predictions forwared through the neural network.\n",
    "        Args:\n",
    "        input_value : Initial input or output value from the previous layer.\n",
    "        w : weight values of the layer's neurons.\n",
    "        b : bias values of the layer's neurons.\n",
    "        Return:\n",
    "        Output value from the layer.\n",
    "    '''\n",
    "    Z = (input_value @ W) + b \n",
    "    A = activation_function(Z)\n",
    "    return A\n",
    "\n",
    "\n",
    "A_layer1 = forward_propagation((prepared_input), W1tf,b1tf)\n",
    "\n",
    "A_layer2 = forward_propagation(A_layer1,W2tf,b2tf)\n",
    "\n",
    "Prediction = forward_propagation(A_layer2,W3tf,b3tf)\n",
    "\n",
    "# Note : You can use variable.shape to check that matrix sizes\n",
    "# matches if you are not sure about how the activation or weights\n",
    "# interact :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first epoch here would mean that for the array of X values we passed, all the Y values corresponding to those X values would be 0.19. Of course, this is the first pass and it is very far from what we expect, and that is why now we have to proceed with the backwards propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mae_error(prediction,true_value):\n",
    "      ''' \n",
    "      This function calculates the mean average error from the predictions we made \n",
    "      and the expected value.\n",
    "      Args:\n",
    "      prediction: prediction value obtained in the forward pass.\n",
    "      true_value: actual value from the input we used.\n",
    "      Return:\n",
    "      mean_errors: mean average error.\n",
    "      '''\n",
    "      m = true_value.shape[0]\n",
    "      loss = tf.abs(true_value - prediction)\n",
    "      mean_loss = tf.reduce_sum(loss) / m\n",
    "\n",
    "      return mean_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to prepare the shape for the y_train now. We use PrepareInput function we created before to have the (9,1) shape needed for doing later the backward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9, 1), dtype=float32, numpy=\n",
       "array([[67.],\n",
       "       [71.],\n",
       "       [75.],\n",
       "       [79.],\n",
       "       [83.],\n",
       "       [87.],\n",
       "       [91.],\n",
       "       [95.],\n",
       "       [99.]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_example = y_train[50:]\n",
    "true_value = PrepareInput(output_example)\n",
    "true_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=81.83067>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae = calculate_mae_error(Prediction,true_value)\n",
    "mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see we have a big error. This is completely normal, as for the first time we run the  `forward propagation`we dont have any information in our weights to know what the prediction should look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imitating backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have calculated the error, we can proceed with the `Backwards pass`. Bringing back the mathematical formula, this is how we update the weights for our dense layers\n",
    "\n",
    "\n",
    "$W ← w - η \\small\\frac{\\partial L}{\\partial W}$\n",
    "\n",
    "​Where:\n",
    "* W is the updated weight.\n",
    "* w is the previous weight.\n",
    "* η is the learning rate.\n",
    "* $ \\small\\frac{\\partial L}{\\partial W}$ is the loss gradient with respect to the weight.\n",
    "\n",
    "We need to clarify now what is exactly the loss gradient. The loss gradient is obtained by multiplying the computed loss by the activation output from the previous layer transposed.\n",
    "\n",
    "$ \\small\\frac{\\partial L}{\\partial W_n} = dZ_n ⋅ A^T_{n-1} $\n",
    "​\n",
    "\n",
    "In the third layer, the computed gradient is the mean averague value error between the true value and the prediction. \n",
    "\n",
    "$ dZ_3=f'( y_{pred} -y_{true} ) $\n",
    "\n",
    "\n",
    "And how do we calculate the local gradient from the current layer ? we do it following this expression:\n",
    "\n",
    "$  dZ_n =(dZ_{n+1}⋅W^T_{n+1})⋅f′(Z_n) $\n",
    "\n",
    "​Where:\n",
    "* $dZ_{n+1}$ is the gradient loss with the respect the output of the posterior layer.\n",
    "\n",
    "* $W^T_{n+1}$ is the transposed weight matrix from the third layer.\n",
    "\n",
    "* $f′(Z_n)$ is the derivative of the activation function from the output obtained from the current layer.\n",
    "\n",
    "It is a bit cumbersome but I hope it makes sense, and now with the code written below it might be easier to see.\n",
    "\n",
    "We specify the learning rate, and we choose a default η= 0.01. We only need to calculate the gradient loss for each layer and apply the derivative function to the output obtained from each layer in order to update our weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ObtainLocalGradient(dZ_post,Z,post_layer_weight,derivative_function=linear_derivative):\n",
    "    '''Function to calculate the local gradient for the first and second layer.\n",
    "\n",
    "    Args:\n",
    "    dZ_post : loss gradient from the layer that goes after the current layer.\n",
    "    Z : result from the activation function we had in the forward propagation\n",
    "        from this layer before.\n",
    "    post_layer_weight: weights from the posterior layer.\n",
    "\n",
    "    return:\n",
    "    dZ : gradient of the loss function with respect to the current laayer.\n",
    "    '''\n",
    "\n",
    "    dA = tf.matmul(dZ_post,post_layer_weight,transpose_b=True)\n",
    "    f_Z = derivative_function(Z)\n",
    "    dZ = dA * f_Z\n",
    "\n",
    "    return dZ\n",
    "\n",
    "def UpdatedWeightValue(dZ,Aminus,weight,n=0.0001):\n",
    "    '''Function used to calculate the updated value of the weights.\n",
    "\n",
    "    Args:\n",
    "    dZ: loss gradient\n",
    "    n: learning rate\n",
    "    x: weight \n",
    "\n",
    "    return:\n",
    "    update: updated weights.\n",
    "    '''\n",
    "\n",
    "\n",
    "    dW= tf.matmul(Aminus,dZ,transpose_a=True)/ tf.cast(tf.shape(Aminus)[0], tf.float32) \n",
    "    weight -= n* dW\n",
    "\n",
    "    return weight\n",
    "\n",
    "def UpdatedBiasValue(dZ,bias,n=0.0001):\n",
    "    '''Function used to calculate the updated value of the biases.\n",
    "    \n",
    "    Args:\n",
    "    dZ: loss gradient\n",
    "    n: learning rate\n",
    "    x: weight or bias\n",
    "\n",
    "    return:\n",
    "    update: updated biases.\n",
    "    '''\n",
    "\n",
    "    db = tf.reduce_sum(dZ, axis=0, keepdims=True)\n",
    "    bias -= n* db\n",
    "    \n",
    "    return bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For  $dZ_{3}$, we have that the local gradient is  $f′(Z_3)$.\n",
    "$(Z_3 = mae)$, and because the derivative of the linear function returns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9, 1), dtype=float32, numpy=\n",
       "array([[-66.064545],\n",
       "       [-70.00608 ],\n",
       "       [-73.94761 ],\n",
       "       [-77.889145],\n",
       "       [-81.83068 ],\n",
       "       [-85.77222 ],\n",
       "       [-89.713745],\n",
       "       [-93.65528 ],\n",
       "       [-97.59682 ]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dZ3= Prediction - true_value\n",
    "dZ3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9, 2), dtype=float32, numpy=\n",
       "array([[  2.7326677,  -8.254143 ],\n",
       "       [  2.9034595,  -8.770027 ],\n",
       "       [  3.074251 ,  -9.285911 ],\n",
       "       [  3.2450428,  -9.801795 ],\n",
       "       [  3.4158344, -10.3176775],\n",
       "       [  3.5866263, -10.833563 ],\n",
       "       [  3.7574182, -11.349445 ],\n",
       "       [  3.9282095, -11.86533  ],\n",
       "       [  4.0990014, -12.381214 ]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new weights for layer 3:  tf.Tensor(\n",
      "[[-0.26764387]\n",
      " [-0.2971279 ]], shape=(2, 1), dtype=float32)\n",
      "new bias for layer 3:  tf.Tensor([[0.0736476]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Layer 3\n",
    "\n",
    "new_wtf3 = UpdatedWeightValue(dZ3,A_layer2,W3tf)\n",
    "new_btf3 = UpdatedBiasValue(dZ3,b3tf)\n",
    "print(\"new weights for layer 3: \",new_wtf3)\n",
    "print(\"new bias for layer 3: \",new_btf3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do the same for the other two layers. First, we obtain the local gradient for the second layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9, 2), dtype=float32, numpy=\n",
       "array([[19.558054, 13.962216],\n",
       "       [20.724926, 14.795229],\n",
       "       [21.891794, 15.62824 ],\n",
       "       [23.058664, 16.461252],\n",
       "       [24.225534, 17.294264],\n",
       "       [25.392406, 18.127275],\n",
       "       [26.559275, 18.960287],\n",
       "       [27.726145, 19.793299],\n",
       "       [28.893015, 20.626312]], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Local gradient for the second layer\n",
    "\n",
    "dZ2 = ObtainLocalGradient(dZ3,A_layer2,W3tf)\n",
    "dZ2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new weights for layer 2:  tf.Tensor(\n",
      "[[-0.2668967  -0.19053374]\n",
      " [ 0.03143832  1.5313946 ]], shape=(2, 2), dtype=float32)\n",
      "new bias for layer 2:  tf.Tensor([[-0.02180298 -0.01556484]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Layer 2\n",
    "\n",
    "new_wtf2 = UpdatedWeightValue(dZ2,A_layer1,W2tf)\n",
    "new_btf2 = UpdatedBiasValue(dZ2,b2tf)\n",
    "\n",
    "print(\"new weights for layer 2: \",new_wtf2)\n",
    "print(\"new bias for layer 2: \",new_btf2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And doing it for the first layer finally would update the weights for the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new weights for layer 1:  tf.Tensor([[-0.25204194 -0.3189828 ]], shape=(1, 2), dtype=float32)\n",
      "new bias for layer 1:  tf.Tensor([[ 0.00974416 -0.02383646]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Local gradient for the first layer\n",
    "\n",
    "dZ1 = ObtainLocalGradient(dZ2,A_layer1,W2tf)\n",
    "\n",
    "#Layer 1\n",
    "\n",
    "new_wtf1 = UpdatedWeightValue(dZ1,prepared_input,W1tf)\n",
    "new_btf1 = UpdatedBiasValue(dZ1,b1tf)\n",
    "\n",
    "print(\"new weights for layer 1: \",new_wtf1)\n",
    "print(\"new bias for layer 1: \",new_btf1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would have updated all weights and complete a first epoch. This is what we are doing internally with the tensorflow library when we train a neural network : we pass through the data with the weights we have and using the activation function we have prefered to use for our experiment. Then, we obtain a prediction that we use to compare to the actual results we know beforehand with the real values. After obtaining this value, we calculate de error gradient and local gradient and pass it backwards, updating the values of the weights to maintain the weights that have helped to achieve the desired results and tune the ones that get our value off from the true values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting all together, running epochs\n",
    "\n",
    "Let's try now to build up a function to run forward propagation and back propagation progressively, and see if we can manage to obtain similar results to the ones obtained when creating a model in tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a class now that contains all the functions we have been creating before, so it is more organized and it is easier to call for the forward and back propagation processes. We still use the function we have created before, we just call them inside our class.\n",
    "\n",
    "Notice how we also save variables in the class so it is also easy to pass internally certain variables to run a full epoch, and how we also create a method to train our new neural network class! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the neural network with given input, hidden, and output sizes.\n",
    "        \"\"\"\n",
    "\n",
    "        # Call the function to initialize weights and biases\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        \"\"\"\n",
    "        Initializes the weights and biases for the 3 layers\n",
    "        \"\"\"\n",
    "        # Layer 1\n",
    "\n",
    "        self.W1,self.b1 = LayerCreation(1,2)\n",
    "\n",
    "        # Layer 2\n",
    "\n",
    "        self.W2,self.b2 = LayerCreation(2,2)\n",
    "\n",
    "        # Layer 3\n",
    "\n",
    "        self.W3,self.b3 = LayerCreation(2,1)\n",
    "\n",
    "        self.initialize_predictions()\n",
    "\n",
    "    def self_forward_propagation(self, input):\n",
    "        \"\"\"\n",
    "        Performs forward propagation to get the predictions.\n",
    "\n",
    "        Args:\n",
    "        input : input array with values with (n,1) shape.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        A_layer1 = forward_propagation(input, self.W1,self.b1)\n",
    "        self.A_layer1 = A_layer1\n",
    "\n",
    "        A_layer2 = forward_propagation(A_layer1,self.W2,self.b2)\n",
    "        self.A_layer2 = A_layer2\n",
    "\n",
    "        self.y_pred = forward_propagation(A_layer2,self.W3,self.b3)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def self_backward_propagation(self,input, y_true):\n",
    "        \"\"\"\n",
    "        Performs backward propagation to update the weights and biases and calculates\n",
    "        the mae.\n",
    "        \n",
    "        Args:\n",
    "        input : input array with values with (n,1) shape.\n",
    "        y_true : array with the true values for the labels.\n",
    "\n",
    "        return:\n",
    "        mae : mean average error calculated from y_prediction and y_true.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        mae = calculate_mae_error(self.y_pred,y_true)\n",
    "\n",
    "        # Update values for layer 3\n",
    "\n",
    "        dZ3= self.y_pred - y_true\n",
    "\n",
    "        new_w3 = UpdatedWeightValue(dZ3,self.A_layer2,self.W3)\n",
    "        new_b3 = UpdatedBiasValue(dZ3,self.b3)\n",
    "\n",
    "        # Update values for layer 2\n",
    "\n",
    "        dZ2 = ObtainLocalGradient(dZ3,self.A_layer2,self.W3)\n",
    "\n",
    "        new_w2 = UpdatedWeightValue(dZ2,self.A_layer1,self.W2)\n",
    "        new_b2 = UpdatedBiasValue(dZ2,self.b3)\n",
    "\n",
    "        # Update values for layer 1\n",
    "\n",
    "        dZ1 = ObtainLocalGradient(dZ2,self.A_layer1,self.W2)\n",
    "\n",
    "        new_w1 = UpdatedWeightValue(dZ1,input,self.W1)\n",
    "        new_b1 = UpdatedBiasValue(dZ1,self.b1)\n",
    "\n",
    "        # Update values\n",
    "\n",
    "        self.update_self_weights(new_w3,new_b3,new_w2,new_b2,new_w1,new_b1)\n",
    "        self.initialize_predictions()\n",
    "        \n",
    "        return mae\n",
    "    \n",
    "    def update_self_weights(self,new_w3,new_b3,new_w2,new_b2,new_w1,new_b1):\n",
    "        \"\"\"\n",
    "        Stores the new values of weights and biases.\n",
    "\n",
    "        Args:\n",
    "        new_wN : new weights calculated to be updated to the weight values of the N layer.\n",
    "        new_bn : new biases calculated to be updated to the bias values of the N layer.\n",
    "\n",
    "        \"\"\"\n",
    "        self.W3 = new_w3\n",
    "        self.b3 = new_b3\n",
    "        self.W2 = new_w2\n",
    "        self.b2 = new_b2\n",
    "        self.W1 = new_w1\n",
    "        self.b1 = new_b1\n",
    "    \n",
    "    def train_network(self,input,y_true,number_epoch):\n",
    "        \"\"\"\n",
    "        Runs a certain amount of epochs to train the network.\n",
    "\n",
    "        Args:\n",
    "        input : input array with values with (n,1) shape.\n",
    "        y_true : array with the true values for the labels. \n",
    "        numer_epoch: number of epochs to run to train the network with the same values introduced as input and y_true.      \n",
    "        \n",
    "        \"\"\"\n",
    "        for epoch in range(number_epoch):\n",
    "\n",
    "            self.self_forward_propagation(input)\n",
    "            mae = self.self_backward_propagation(input,y_true)\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                # Print the loss every 10 iterations\n",
    "                print(f\"Epoch {epoch+1}/{number_epoch}, Loss: {mae}\")\n",
    "                \n",
    "    def do_prediction(self,input):\n",
    "\n",
    "        \"\"\"\n",
    "        Runs a forward prop to determine the expected value for a certain input.\n",
    "\n",
    "        Args:\n",
    "        input : input array with values with (n,1) shape.\n",
    "\n",
    "        return:\n",
    "        self.y_pred =  y_prediction value stored in the class from doing a forward propagation with the specific input value passed before.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        self.self_forward_propagation(input)\n",
    "            \n",
    "        return self.y_pred\n",
    "\n",
    "    def show_weights(self):\n",
    "        \"\"\"\n",
    "        Prints weight values\n",
    "        \n",
    "        \"\"\"\n",
    "        print(\"Weights from first layer : \",self.W1)\n",
    "        print(\"Weights from second layer : \",self.W2)\n",
    "        print(\"Weights from third layer : \",self.W3)\n",
    "\n",
    "    def show_biases(self):\n",
    "        \"\"\"\n",
    "        Prints bias values\n",
    "        \n",
    "        \"\"\"\n",
    "        print(\"Biases from first layer : \",self.b1)\n",
    "        print(\"Biases from second layer : \",self.b2)\n",
    "        print(\"Biases from third layer : \",self.b3)    \n",
    "\n",
    "    def get_prediction(self):\n",
    "        \"\"\"\n",
    "        returns prediction stored.\n",
    "        \n",
    "        \"\"\"\n",
    "        return self.y_pred\n",
    "    \n",
    "    def initialize_predictions(self):\n",
    "        \"\"\"\n",
    "        Initializes prediction class variable. Must do when creating the NN and after each epoch.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.y_prediction=[]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by initializing our network, doing an instantiation of OurNeuralNetwork class. In order to simplify and to demostrate a similar network to the one we created with tensorflow, we have the number of layers and neurons called from within the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_network = OurNeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights from first layer :  tf.Tensor([[-0.29604465 -0.21134205]], shape=(1, 2), dtype=float32)\n",
      "Weights from second layer :  tf.Tensor(\n",
      "[[-0.29604465 -0.21134205]\n",
      " [ 0.01063002  1.5165398 ]], shape=(2, 2), dtype=float32)\n",
      "Weights from third layer :  tf.Tensor(\n",
      "[[-0.29604465]\n",
      " [-0.21134205]], shape=(2, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "our_network.show_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to do a forward and backward pass. As you can see, within these three lines we are calling and repeating the same thing we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_network.self_forward_propagation(prepared_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=81.83067>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_network.self_backward_propagation(prepared_input,true_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling it again, we are doing another epoch. Each time at the end of backward propagation, we are updating the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=78.28142>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_network.self_forward_propagation(prepared_input)\n",
    "our_network.self_backward_propagation(prepared_input,true_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check the weights, we can see how they have been changing during these processes already. We are already training our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights from first layer :  tf.Tensor([[-0.21130395 -0.46642485]], shape=(1, 2), dtype=float32)\n",
      "Weights from second layer :  tf.Tensor(\n",
      "[[-0.24545546 -0.16673051]\n",
      " [ 0.05865001  1.561604  ]], shape=(2, 2), dtype=float32)\n",
      "Weights from third layer :  tf.Tensor(\n",
      "[[-0.24962954]\n",
      " [-0.43768468]], shape=(2, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "our_network.show_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a proper training, and see how the prediction turn to be. Let's get back to our training and testing samples, and do their preparation before feeding them into our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_x_train= PrepareInput(x_train)\n",
    "prep_y_train= PrepareInput(y_train)\n",
    "\n",
    "prep_x_test= PrepareInput(x_test)\n",
    "prep_y_test= PrepareInput(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30, Loss: 2.698012113571167\n",
      "Epoch 20/30, Loss: 2.2568602561950684\n",
      "Epoch 30/30, Loss: 1.8855582475662231\n"
     ]
    }
   ],
   "source": [
    "our_network.train_network(prep_x_train,prep_y_train,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have achieved a good loss and the weights are trained properly in our simple neural network made from scratch. What if we try to predict how the y_test looks from x_test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(16, 1), dtype=float32, numpy=\n",
       " array([[-197.42119],\n",
       "        [-193.45299],\n",
       "        [-189.48479],\n",
       "        [-185.51656],\n",
       "        [-181.54836],\n",
       "        [-177.58015],\n",
       "        [-173.61194],\n",
       "        [-169.64374],\n",
       "        [-165.67554],\n",
       "        [-161.70732],\n",
       "        [-157.7391 ],\n",
       "        [-153.7709 ],\n",
       "        [-149.8027 ],\n",
       "        [-145.83447],\n",
       "        [-141.86629],\n",
       "        [-137.89807]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(16, 1), dtype=float32, numpy=\n",
       " array([[-197.],\n",
       "        [-193.],\n",
       "        [-189.],\n",
       "        [-185.],\n",
       "        [-181.],\n",
       "        [-177.],\n",
       "        [-173.],\n",
       "        [-169.],\n",
       "        [-165.],\n",
       "        [-161.],\n",
       "        [-157.],\n",
       "        [-153.],\n",
       "        [-149.],\n",
       "        [-145.],\n",
       "        [-141.],\n",
       "        [-137.]], dtype=float32)>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = our_network.do_prediction(prep_x_test)\n",
    "y_test_pred,prep_y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model looks succesful and see how we managed to obtain such a perfect prediction model with just a few training epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try now to do the same with another neural network, for a similar correlation problem and see the results as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_new = np.arange(0,10,1)\n",
    "y_new = OurLinearCorrelation(x_new)\n",
    "x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_prep = PrepareInput(x_new)\n",
    "y_prep = PrepareInput(y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we tried to see how our previous model looks when predicting this new set ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
       " array([[ 0.9892023],\n",
       "        [ 2.9733064],\n",
       "        [ 4.957411 ],\n",
       "        [ 6.941515 ],\n",
       "        [ 8.925619 ],\n",
       "        [10.909723 ],\n",
       "        [12.8938265],\n",
       "        [14.877931 ],\n",
       "        [16.862034 ],\n",
       "        [18.846142 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
       " array([[ 3.],\n",
       "        [ 5.],\n",
       "        [ 7.],\n",
       "        [ 9.],\n",
       "        [11.],\n",
       "        [13.],\n",
       "        [15.],\n",
       "        [17.],\n",
       "        [19.],\n",
       "        [21.]], dtype=float32)>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = our_network.do_prediction(x_prep)\n",
    "y_test_pred,y_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how our model as it was training on the features of the other function, does not retrieve the same values as with the function it was trained with. We could do two things now: retrain our old model and adapt it to the new correlation function we created, or create a new one.\n",
    "\n",
    "This is for now an example of how to navigate through neural networks, and how to understand better what is happening in between the layers when we work in Tensorflow. In future notebooks, we will try to have a deeper look on more complex networks to see how these interactions are. In the end, what we need to have clear are these rules we have seen here, as with more complex networks what we have in the end are larger matrix indexes, but the idea stays the same."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
